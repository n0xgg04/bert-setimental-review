{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n lo·∫°i c·∫£m x√∫c IMDB s·ª≠ d·ª•ng BiLSTM + Attention\n",
    "\n",
    "Trong notebook n√†y, ch√∫ng ta s·∫Ω x√¢y d·ª±ng m√¥ h√¨nh BiLSTM k·∫øt h·ª£p v·ªõi l·ªõp Attention ƒë·ªÉ ph√¢n lo·∫°i c·∫£m x√∫c (t√≠ch c·ª±c/ti√™u c·ª±c) tr√™n b·ªô d·ªØ li·ªáu IMDb movie reviews.\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- X√¢y d·ª±ng m√¥ h√¨nh BiLSTM + Attention t·ª´ ƒë·∫ßu\n",
    "- Hu·∫•n luy·ªán tr√™n dataset IMDb \n",
    "- ƒê√°nh gi√° hi·ªáu qu·∫£ m√¥ h√¨nh\n",
    "- So s√°nh v·ªõi c√°c ph∆∞∆°ng ph√°p kh√°c\n",
    "\n",
    "## C√°c b∆∞·ªõc th·ª±c hi·ªán:\n",
    "1. Import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng\n",
    "2. Load v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu IMDb\n",
    "3. X√¢y d·ª±ng l·ªõp Attention t√πy ch·ªânh\n",
    "4. T·∫°o m√¥ h√¨nh BiLSTM + Attention\n",
    "5. Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "6. ƒê√°nh gi√° v√† tr·ª±c quan h√≥a k·∫øt qu·∫£\n",
    "7. Test v·ªõi d·ªØ li·ªáu m·ªõi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ae569",
   "metadata": {},
   "source": [
    "### 1. Import th∆∞ vi·ªán v√† thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng\n",
    "\n",
    "Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho vi·ªác x√¢y d·ª±ng m√¥ h√¨nh BiLSTM + Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d145113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thi·∫øt l·∫≠p seed cho reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä System Information:\n",
      "TensorFlow version: 2.13.0\n",
      "‚ùå No GPU detected - using CPU\n",
      "üí° Consider using Google Colab for GPU access\n"
     ]
    }
   ],
   "source": [
    "def setup_gpu():\n",
    "    \"\"\"\n",
    "    Setup v√† t·ªëi ∆∞u GPU cho training\n",
    "    \"\"\"\n",
    "    print(\"üìä System Information:\")\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(f\"‚úÖ GPU detected: {len(gpus)} GPU(s)\")\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            print(f\"   GPU {i}: {gpu}\")\n",
    "        \n",
    "        try:\n",
    "            # Enable memory growth ƒë·ªÉ tr√°nh chi·∫øm h·∫øt VRAM\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(\"‚úÖ GPU memory growth enabled\")\n",
    "            \n",
    "            # Enable mixed precision cho training nhanh h∆°n\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "            print(\"‚úÖ Mixed precision (float16) enabled - 2x faster training!\")\n",
    "            \n",
    "            # Set primary GPU\n",
    "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "            print(f\"‚úÖ Using GPU: {gpus[0]}\")\n",
    "            \n",
    "            # Test GPU\n",
    "            with tf.device('/GPU:0'):\n",
    "                test_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "                result = tf.matmul(test_tensor, test_tensor)\n",
    "            print(f\"‚úÖ GPU test successful: {result.device}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            print(f\"‚ùå GPU setup failed: {e}\")\n",
    "            print(\"‚ö†Ô∏è  Falling back to CPU\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ùå No GPU detected - using CPU\")\n",
    "        print(\"üí° Consider using Google Colab for GPU access\")\n",
    "        return False\n",
    "    \n",
    "gpu_available = setup_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8f5b7",
   "metadata": {},
   "source": [
    "### 2. T·∫°o l·ªõp Attention t√πy ch·ªânh\n",
    "\n",
    "X√¢y d·ª±ng l·ªõp Attention ƒë·ªÉ tƒÉng kh·∫£ nƒÉng t·∫≠p trung v√†o nh·ªØng ph·∫ßn quan tr·ªçng trong chu·ªói vƒÉn b·∫£n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c620a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L·ªõp AttentionLayer optimized ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Attention Layer cho BiLSTM - Optimized version\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Ma tr·∫≠n tr·ªçng s·ªë attention v·ªõi L2 regularization\n",
    "        self.W = self.add_weight(name=\"attention_weight\", \n",
    "                                shape=(input_shape[-1], 1),\n",
    "                                initializer='glorot_uniform',  # Better initialization\n",
    "                                regularizer=regularizers.l2(0.01),\n",
    "                                trainable=True)\n",
    "        # Bias cho attention\n",
    "        self.b = self.add_weight(name=\"attention_bias\", \n",
    "                                shape=(input_shape[1], 1),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, training=None):\n",
    "        # x shape: (batch_size, time_steps, features)\n",
    "        # T√≠nh attention scores\n",
    "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        # Chu·∫©n h√≥a attention weights b·∫±ng softmax\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        \n",
    "        # √Åp d·ª•ng attention weights l√™n input\n",
    "        output = x * a\n",
    "        \n",
    "        # T·ªïng h·ª£p th√¥ng tin t·ª´ t·∫•t c·∫£ time steps\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(AttentionLayer, self).get_config()\n",
    "        return config\n",
    "\n",
    "print(\"L·ªõp AttentionLayer optimized ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168b18c",
   "metadata": {},
   "source": [
    "### 3. Load v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu IMDb\n",
    "\n",
    "T·∫£i b·ªô d·ªØ li·ªáu IMDb movie reviews v√† th·ª±c hi·ªán ti·ªÅn x·ª≠ l√Ω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78ae3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(max_features=10000, maxlen=400):  # Gi·∫£m maxlen\n",
    "    \"\"\"\n",
    "    Load v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu IMDb v·ªõi validation split\n",
    "    \"\"\"\n",
    "    print(\"ƒêang t·∫£i d·ªØ li·ªáu IMDb...\")\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    \n",
    "    print(f\"S·ªë l∆∞·ª£ng reviews training: {len(x_train)}\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng reviews testing: {len(x_test)}\")\n",
    "    print(f\"S·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa: {max_features}\")\n",
    "    print(f\"ƒê·ªô d√†i sequence t·ªëi ƒëa: {maxlen}\")\n",
    "    \n",
    "    # Ki·ªÉm tra ph√¢n b·ªë nh√£n\n",
    "    print(f\"\\nPh√¢n b·ªë nh√£n training:\")\n",
    "    print(f\"- Negative (0): {sum(y_train == 0)} reviews\")\n",
    "    print(f\"- Positive (1): {sum(y_train == 1)} reviews\")\n",
    "    \n",
    "    # Padding sequences ƒë·ªÉ c√≥ c√πng ƒë·ªô d√†i\n",
    "    print(\"\\nƒêang padding sequences...\")\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen, padding='post', truncating='post')\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen, padding='post', truncating='post')\n",
    "    \n",
    "    # T·∫°o validation set t·ª´ training set\n",
    "    validation_split = 0.2\n",
    "    val_size = int(len(x_train) * validation_split)\n",
    "    \n",
    "    # Shuffle data tr∆∞·ªõc khi split\n",
    "    indices = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[indices]\n",
    "    y_train_shuffled = y_train[indices]\n",
    "    \n",
    "    # Split data\n",
    "    x_val = x_train_shuffled[:val_size]\n",
    "    y_val = y_train_shuffled[:val_size]\n",
    "    x_train_final = x_train_shuffled[val_size:]\n",
    "    y_train_final = y_train_shuffled[val_size:]\n",
    "    \n",
    "    print(f\"Shape sau padding v√† split:\")\n",
    "    print(f\"x_train: {x_train_final.shape}\")\n",
    "    print(f\"x_val: {x_val.shape}\")\n",
    "    print(f\"x_test: {x_test.shape}\")\n",
    "    \n",
    "    return (x_train_final, y_train_final), (x_val, y_val), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddeae7c",
   "metadata": {},
   "source": [
    "### 4. X√¢y d·ª±ng m√¥ h√¨nh BiLSTM + Attention\n",
    "\n",
    "T·∫°o m√¥ h√¨nh s·ª≠ d·ª•ng l·ªõp BiLSTM k·∫øt h·ª£p v·ªõi Attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a98a9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒêang t·∫£i d·ªØ li·ªáu IMDb...\n",
      "S·ªë l∆∞·ª£ng reviews training: 25000\n",
      "S·ªë l∆∞·ª£ng reviews testing: 25000\n",
      "S·ªë l∆∞·ª£ng t·ª´ t·ªëi ƒëa: 10000\n",
      "ƒê·ªô d√†i sequence t·ªëi ƒëa: 400\n",
      "\n",
      "Ph√¢n b·ªë nh√£n training:\n",
      "- Negative (0): 12500 reviews\n",
      "- Positive (1): 12500 reviews\n",
      "\n",
      "ƒêang padding sequences...\n",
      "Shape sau padding v√† split:\n",
      "x_train: (20000, 400)\n",
      "x_val: (5000, 400)\n",
      "x_test: (25000, 400)\n",
      "ƒêang x√¢y d·ª±ng optimized model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ki·∫øn tr√∫c m√¥ h√¨nh optimized:\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 400, 64)           640000    \n",
      "                                                                 \n",
      " spatial_dropout1d_3 (Spati  (None, 400, 64)           0         \n",
      " alDropout1D)                                                    \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 400, 64)           24832     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 400, 32)           10368     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " attention_layer_3 (Attenti  (None, 32)                432       \n",
      " onLayer)                                                        \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 32)                128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677489 (2.58 MB)\n",
      "Trainable params: 677361 (2.58 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "\n",
    "def create_gpu_optimized_bilstm_model(max_features, maxlen, \n",
    "                                     embedding_dim=64, lstm_units=32):\n",
    "    \"\"\"\n",
    "    T·∫°o model BiLSTM v·ªõi Attention - GPU optimized\n",
    "    \"\"\"\n",
    "    print(\"\\nüèóÔ∏è  Building GPU-optimized BiLSTM + Attention model...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    with tf.device('/GPU:0' if gpu_available else '/CPU:0'):\n",
    "        model = models.Sequential()\n",
    "        \n",
    "        # L·ªõp Embedding v·ªõi regularization\n",
    "        model.add(layers.Embedding(\n",
    "            max_features, \n",
    "            embedding_dim, \n",
    "            input_length=maxlen,\n",
    "            embeddings_regularizer=regularizers.l2(0.001),\n",
    "            name='embedding'\n",
    "        ))\n",
    "        model.add(layers.SpatialDropout1D(0.3, name='spatial_dropout'))\n",
    "        \n",
    "        # BiLSTM layers v·ªõi GPU optimization\n",
    "        model.add(layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units, \n",
    "                       return_sequences=True, \n",
    "                       dropout=0.4,\n",
    "                       recurrent_dropout=0.4,\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       recurrent_regularizer=regularizers.l2(0.01)),\n",
    "            name='bidirectional_lstm_1'\n",
    "        ))\n",
    "        \n",
    "        model.add(layers.Bidirectional(\n",
    "            layers.LSTM(lstm_units//2,\n",
    "                       return_sequences=True, \n",
    "                       dropout=0.4,\n",
    "                       recurrent_dropout=0.4,\n",
    "                       kernel_regularizer=regularizers.l2(0.01),\n",
    "                       recurrent_regularizer=regularizers.l2(0.01)),\n",
    "            name='bidirectional_lstm_2'\n",
    "        ))\n",
    "        \n",
    "        # Attention layer\n",
    "        model.add(AttentionLayer(name='attention'))\n",
    "        model.add(layers.BatchNormalization(name='batch_norm_1'))\n",
    "        model.add(layers.Dropout(0.5, name='dropout_1'))\n",
    "        \n",
    "        # Dense layers\n",
    "        model.add(layers.Dense(32, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(0.01),\n",
    "                              name='dense_1'))\n",
    "        model.add(layers.BatchNormalization(name='batch_norm_2'))\n",
    "        model.add(layers.Dropout(0.4, name='dropout_2'))\n",
    "        \n",
    "        model.add(layers.Dense(16, activation='relu',\n",
    "                              kernel_regularizer=regularizers.l2(0.01),\n",
    "                              name='dense_2'))\n",
    "        model.add(layers.Dropout(0.3, name='dropout_3'))\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(layers.Dense(1, activation='sigmoid', name='output'))\n",
    "    \n",
    "    print(f\"‚úÖ Model created on: {'GPU' if gpu_available else 'CPU'}\")\n",
    "    return model\n",
    "\n",
    "def create_optimized_bilstm_attention_model(max_features, maxlen, \n",
    "                                          embedding_dim=64,    # Gi·∫£m t·ª´ 128\n",
    "                                          lstm_units=32):      # Gi·∫£m t·ª´ 64\n",
    "    \"\"\"\n",
    "    T·∫°o model BiLSTM v·ªõi Attention - Optimized ƒë·ªÉ tr√°nh overfitting\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # L·ªõp Embedding v·ªõi regularization\n",
    "    model.add(layers.Embedding(\n",
    "        max_features, \n",
    "        embedding_dim, \n",
    "        input_length=maxlen,\n",
    "        embeddings_regularizer=regularizers.l2(0.001)  # L2 regularization\n",
    "    ))\n",
    "    model.add(layers.SpatialDropout1D(0.3))  # Spatial dropout cho embedding\n",
    "    \n",
    "    # BiLSTM layers v·ªõi √≠t units h∆°n v√† regularization m·∫°nh h∆°n\n",
    "    model.add(layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units, \n",
    "                   return_sequences=True, \n",
    "                   dropout=0.4,                    # TƒÉng dropout\n",
    "                   recurrent_dropout=0.4,          # TƒÉng recurrent dropout\n",
    "                   kernel_regularizer=regularizers.l2(0.01),  # L2 regularization\n",
    "                   recurrent_regularizer=regularizers.l2(0.01))\n",
    "    ))\n",
    "    \n",
    "    # Ch·ªâ m·ªôt BiLSTM layer thay v√¨ hai ƒë·ªÉ gi·∫£m complexity\n",
    "    model.add(layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units//2,               # Gi·∫£m units ·ªü layer th·ª© 2\n",
    "                   return_sequences=True, \n",
    "                   dropout=0.4,\n",
    "                   recurrent_dropout=0.4,\n",
    "                   kernel_regularizer=regularizers.l2(0.01),\n",
    "                   recurrent_regularizer=regularizers.l2(0.01))\n",
    "    ))\n",
    "    \n",
    "    # L·ªõp Attention\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.BatchNormalization())      # Batch normalization\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Dense layers v·ªõi √≠t neurons h∆°n\n",
    "    model.add(layers.Dense(32,                  # Gi·∫£m t·ª´ 64\n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(layers.Dense(16,                  # Gi·∫£m t·ª´ 32\n",
    "                          activation='relu',\n",
    "                          kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # L·ªõp output\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Tham s·ªë m√¥ h√¨nh\n",
    "MAX_FEATURES = 10000\n",
    "MAXLEN = 400          # Gi·∫£m t·ª´ 500\n",
    "EMBEDDING_DIM = 64    # Gi·∫£m t·ª´ 128\n",
    "LSTM_UNITS = 32       # Gi·∫£m t·ª´ 64\n",
    "\n",
    "# Load d·ªØ li·ªáu v·ªõi validation split\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_and_preprocess_data(MAX_FEATURES, MAXLEN)\n",
    "\n",
    "# T·∫°o m√¥ h√¨nh optimized\n",
    "print(\"ƒêang x√¢y d·ª±ng optimized model...\")\n",
    "model = create_optimized_bilstm_attention_model(MAX_FEATURES, MAXLEN, EMBEDDING_DIM, LSTM_UNITS) if not gpu_available else create_gpu_optimized_bilstm_model(MAX_FEATURES, MAXLEN, EMBEDDING_DIM, LSTM_UNITS)\n",
    "\n",
    "# Compile v·ªõi learning rate scheduler\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Hi·ªÉn th·ªã ki·∫øn tr√∫c m√¥ h√¨nh\n",
    "print(\"\\nKi·∫øn tr√∫c m√¥ h√¨nh optimized:\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e695efa",
   "metadata": {},
   "source": [
    "### 5. Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "\n",
    "Thi·∫øt l·∫≠p callbacks v√† b·∫Øt ƒë·∫ßu qu√° tr√¨nh hu·∫•n luy·ªán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd08769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán optimized model...\n",
      "Batch size: 64\n",
      "Max epochs: 15\n",
      "Validation split: Separate validation set\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax epochs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation split: Separate validation set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mHu·∫•n luy·ªán ho√†n t·∫•t!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn\u001b[38;5;241m.\u001b[39m_function_spec  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# TƒÉng batch size cho GPU ƒë·ªÉ t·∫≠n d·ª•ng parallel processing\n",
    "BATCH_SIZE = 128 if gpu_available else 32  # GPU: 128, CPU: 32\n",
    "EPOCHS = 15\n",
    "\n",
    "# Callbacks ƒë∆∞·ª£c c·∫£i thi·ªán\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',     # Monitor validation accuracy\n",
    "        patience=4,                 # TƒÉng patience\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=0.3,                # Gi·∫£m learning rate m·∫°nh h∆°n\n",
    "        patience=2, \n",
    "        min_lr=0.00001,            # Lower minimum learning rate\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_bilstm_attention_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán\n",
    "print(\"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán optimized model...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Validation split: Separate validation set\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nHu·∫•n luy·ªán ho√†n t·∫•t!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202f23e",
   "metadata": {},
   "source": [
    "### 6. Tr·ª±c quan h√≥a qu√° tr√¨nh hu·∫•n luy·ªán\n",
    "\n",
    "V·∫Ω bi·ªÉu ƒë·ªì ƒë·ªÉ quan s√°t qu√° tr√¨nh hu·∫•n luy·ªán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32547282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh training\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax1.set_title('ƒê·ªô ch√≠nh x√°c m√¥ h√¨nh qua c√°c epoch')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax2.set_title('Loss m√¥ h√¨nh qua c√°c epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# V·∫Ω bi·ªÉu ƒë·ªì\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99231b",
   "metadata": {},
   "source": [
    "### 7. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "\n",
    "ƒê√°nh gi√° hi·ªáu qu·∫£ m√¥ h√¨nh tr√™n t·∫≠p test v√† hi·ªÉn th·ªã c√°c metrics chi ti·∫øt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° model v√† hi·ªÉn th·ªã k·∫øt qu·∫£ chi ti·∫øt\n",
    "    \"\"\"\n",
    "    # D·ª± ƒëo√°n tr√™n t·∫≠p test\n",
    "    print(\"ƒêang th·ª±c hi·ªán d·ª± ƒëo√°n tr√™n t·∫≠p test...\")\n",
    "    y_pred_prob = model.predict(x_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"B√ÅO C√ÅO PH√ÇN LO·∫†I CHI TI·∫æT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Ma tr·∫≠n nh·∫ßm l·∫´n (Confusion Matrix)')\n",
    "    plt.xlabel('D·ª± ƒëo√°n')\n",
    "    plt.ylabel('Th·ª±c t·∫ø')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"K·∫æT QU·∫¢ CU·ªêI C√ôNG\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p test: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"Loss tr√™n t·∫≠p test: {test_loss:.4f}\")\n",
    "    \n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "# ƒê√°nh gi√° m√¥ h√¨nh\n",
    "y_pred, y_pred_prob = evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044dedc",
   "metadata": {},
   "source": [
    "### 8. Test v·ªõi d·ªØ li·ªáu m·ªõi\n",
    "\n",
    "Th·ª≠ nghi·ªám m√¥ h√¨nh v·ªõi m·ªôt s·ªë reviews m·∫´u ƒë·ªÉ xem kh·∫£ nƒÉng d·ª± ƒëo√°n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd39bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, word_index, maxlen=500):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n c·∫£m x√∫c cho m·ªôt text m·ªõi\n",
    "    \"\"\"\n",
    "    # Chuy·ªÉn text th√†nh sequence\n",
    "    sequence = []\n",
    "    for word in text.lower().split():\n",
    "        if word in word_index and word_index[word] < 10000:\n",
    "            sequence.append(word_index[word])\n",
    "    \n",
    "    # N·∫øu sequence r·ªóng, return unknown\n",
    "    if not sequence:\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"Kh√¥ng th·ªÉ x·ª≠ l√Ω text n√†y (kh√¥ng c√≥ t·ª´ n√†o trong t·ª´ ƒëi·ªÉn)\")\n",
    "        print(\"-\" * 50)\n",
    "        return\n",
    "    \n",
    "    # Padding\n",
    "    sequence = pad_sequences([sequence], maxlen=maxlen)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sequence, verbose=0)[0][0]\n",
    "    \n",
    "    sentiment = \"T√≠ch c·ª±c (Positive)\" if prediction > 0.5 else \"Ti√™u c·ª±c (Negative)\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"C·∫£m x√∫c: {sentiment}\")\n",
    "    print(f\"ƒê·ªô tin c·∫≠y: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"ƒêi·ªÉm s·ªë raw: {prediction:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Load word index ƒë·ªÉ chuy·ªÉn ƒë·ªïi text\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print(\"KI·ªÇM TH·ª¨ M√î H√åNH V·ªöI C√ÅC REVIEWS M·∫™U\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test v·ªõi c√°c examples kh√°c nhau\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic! Great acting and amazing storyline.\",\n",
    "    \"Terrible film. Boring plot and bad acting. Complete waste of time.\",\n",
    "    \"The movie was okay, nothing special but not bad either.\",\n",
    "    \"One of the best movies I have ever seen! Highly recommended!\",\n",
    "    \"Awful movie. Poor direction and terrible script. Very disappointed.\",\n",
    "    \"Amazing cinematography and outstanding performances from all actors.\",\n",
    "    \"I fell asleep halfway through. Very boring and predictable plot.\",\n",
    "    \"Perfect movie for the weekend. Really enjoyed watching it.\",\n",
    "    \"Not my cup of tea but can understand why others might like it.\",\n",
    "    \"Masterpiece! Every scene was perfectly crafted and emotionally engaging.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    predict_sentiment(model, text, word_index, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f885f",
   "metadata": {},
   "source": [
    "### 9. L∆∞u m√¥ h√¨nh\n",
    "\n",
    "L∆∞u l·∫°i m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u m√¥ h√¨nh\n",
    "model.save('bilstm_attention_imdb_model.h5')\n",
    "print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh 'bilstm_attention_imdb_model.h5'\")\n",
    "\n",
    "# L∆∞u th√¥ng tin v·ªÅ qu√° tr√¨nh hu·∫•n luy·ªán\n",
    "import json\n",
    "\n",
    "training_info = {\n",
    "    'max_features': MAX_FEATURES,\n",
    "    'maxlen': MAXLEN,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "    'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "    'final_train_loss': float(history.history['loss'][-1]),\n",
    "    'final_val_loss': float(history.history['val_loss'][-1])\n",
    "}\n",
    "\n",
    "with open('training_info.json', 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Th√¥ng tin hu·∫•n luy·ªán ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'training_info.json'\")\n",
    "print(\"\\nTh√¥ng tin m√¥ h√¨nh:\")\n",
    "for key, value in training_info.items():\n",
    "    if 'accuracy' in key or 'loss' in key:\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4b412",
   "metadata": {},
   "source": [
    "### 10. T√≥m t·∫Øt v√† so s√°nh\n",
    "\n",
    "#### ∆Øu ƒëi·ªÉm c·ªßa m√¥ h√¨nh BiLSTM + Attention:\n",
    "- **Hi·ªÉu ng·ªØ c·∫£nh hai chi·ªÅu**: BiLSTM x·ª≠ l√Ω th√¥ng tin t·ª´ c·∫£ hai h∆∞·ªõng (tr∆∞·ªõc v√† sau)\n",
    "- **T·∫≠p trung v√†o ph·∫ßn quan tr·ªçng**: L·ªõp Attention gi√∫p m√¥ h√¨nh ch√∫ √Ω ƒë·∫øn nh·ªØng t·ª´ quan tr·ªçng nh·∫•t\n",
    "- **Hi·ªáu qu·∫£ v·ªõi d·ªØ li·ªáu tu·∫ßn t·ª±**: Ph√π h·ª£p v·ªõi ƒë·∫∑c t√≠nh c·ªßa vƒÉn b·∫£n l√† d·ªØ li·ªáu tu·∫ßn t·ª±\n",
    "- **T·ªëc ƒë·ªô hu·∫•n luy·ªán h·ª£p l√Ω**: Nhanh h∆°n so v·ªõi c√°c m√¥ h√¨nh Transformer l·ªõn\n",
    "\n",
    "#### Nh∆∞·ª£c ƒëi·ªÉm:\n",
    "- **ƒê·ªô ch√≠nh x√°c th·∫•p h∆°n BERT**: C√≥ th·ªÉ kh√¥ng ƒë·∫°t ƒë∆∞·ª£c ƒë·ªô ch√≠nh x√°c cao nh∆∞ c√°c m√¥ h√¨nh pretrained l·ªõn\n",
    "- **C·∫ßn nhi·ªÅu d·ªØ li·ªáu**: Hi·ªáu qu·∫£ t·ªët nh·∫•t khi c√≥ l∆∞·ª£ng d·ªØ li·ªáu hu·∫•n luy·ªán l·ªõn\n",
    "- **Kh√≥ x·ª≠ l√Ω m·ªëi quan h·ªá xa**: LSTM c√≥ th·ªÉ g·∫∑p kh√≥ khƒÉn v·ªõi c√°c ph·ª• thu·ªôc xa trong vƒÉn b·∫£n\n",
    "\n",
    "#### So s√°nh v·ªõi BERT:\n",
    "- **BERT**: ƒê·ªô ch√≠nh x√°c cao h∆°n nh∆∞ng c·∫ßn nhi·ªÅu t√†i nguy√™n t√≠nh to√°n v√† th·ªùi gian hu·∫•n luy·ªán\n",
    "- **BiLSTM + Attention**: C√¢n b·∫±ng gi·ªØa hi·ªáu qu·∫£ v√† t·ªëc ƒë·ªô, ph√π h·ª£p khi t√†i nguy√™n h·∫°n ch·∫ø\n",
    "\n",
    "M√¥ h√¨nh BiLSTM + Attention l√† m·ªôt l·ª±a ch·ªçn t·ªët khi c·∫ßn c√¢n b·∫±ng gi·ªØa ƒë·ªô ch√≠nh x√°c v√† hi·ªáu qu·∫£ t√≠nh to√°n!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4360a",
   "metadata": {},
   "source": [
    "### 11. T·∫°o th∆∞ m·ª•c v√† l∆∞u m√¥ h√¨nh\n",
    "\n",
    "T·∫°o th∆∞ m·ª•c `bilstm` v√† l∆∞u m√¥ h√¨nh c√πng v·ªõi c√°c th√¥ng tin li√™n quan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c bilstm n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('bilstm', exist_ok=True)\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh v√†o th∆∞ m·ª•c bilstm\n",
    "model.save('bilstm/bilstm_attention_model.h5')\n",
    "print(\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'bilstm/bilstm_attention_model.h5'\")\n",
    "\n",
    "# L∆∞u tokenizer info (word_index) ƒë·ªÉ s·ª≠ d·ª•ng sau n√†y\n",
    "import pickle\n",
    "with open('bilstm/word_index.pkl', 'wb') as f:\n",
    "    pickle.dump(word_index, f)\n",
    "print(\"‚úÖ Word index ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'bilstm/word_index.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f366e6",
   "metadata": {},
   "source": [
    "### 12. T√≠nh to√°n c√°c th√¥ng s·ªë ƒë√°nh gi√° chi ti·∫øt\n",
    "\n",
    "#### 12.1 ƒê·ªô ch√≠nh x√°c (Accuracy Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# T√≠nh to√°n c√°c metrics ch√≠nh x√°c\n",
    "print(\"ƒêang t√≠nh to√°n c√°c th√¥ng s·ªë ƒë√°nh gi√°...\")\n",
    "\n",
    "# 1. ACCURACY METRICS\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion Matrix chi ti·∫øt\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# T√≠nh to√°n th√™m c√°c metrics\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)  # = recall\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "false_negative_rate = fn / (fn + tp)\n",
    "\n",
    "print(\"1. ACCURACY METRICS:\")\n",
    "print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   - Precision: {precision:.4f}\")\n",
    "print(f\"   - Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"   - Specificity: {specificity:.4f}\")\n",
    "print(f\"   - F1-Score: {f1:.4f}\")\n",
    "print(f\"   - AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"   - False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"   - False Negative Rate: {false_negative_rate:.4f}\")\n",
    "\n",
    "accuracy_metrics = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'specificity': float(specificity),\n",
    "    'f1_score': float(f1),\n",
    "    'auc_roc': float(auc_roc),\n",
    "    'false_positive_rate': float(false_positive_rate),\n",
    "    'false_negative_rate': float(false_negative_rate),\n",
    "    'true_positives': int(tp),\n",
    "    'true_negatives': int(tn),\n",
    "    'false_positives': int(fp),\n",
    "    'false_negatives': int(fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100d1af",
   "metadata": {},
   "source": [
    "#### 12.2 Hi·ªáu su·∫•t t√≠nh to√°n (Performance Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10283157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PERFORMANCE METRICS\n",
    "\n",
    "# T√≠nh th·ªùi gian training (∆∞·ªõc l∆∞·ª£ng t·ª´ history)\n",
    "epochs_trained = len(history.history['loss'])\n",
    "# ∆Ø·ªõc l∆∞·ª£ng th·ªùi gian training (c√≥ th·ªÉ kh√¥ng ch√≠nh x√°c 100%)\n",
    "estimated_training_time = epochs_trained * 60  # ∆Ø·ªõc l∆∞·ª£ng 60s/epoch\n",
    "\n",
    "# ƒêo th·ªùi gian inference\n",
    "print(\"ƒêang ƒëo th·ªùi gian inference...\")\n",
    "inference_times = []\n",
    "batch_size_test = 100\n",
    "\n",
    "for i in range(5):  # ƒêo 5 l·∫ßn ƒë·ªÉ l·∫•y trung b√¨nh\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(x_test[:batch_size_test], verbose=0)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "inference_time_per_sample = avg_inference_time / batch_size_test\n",
    "\n",
    "# ƒêo memory usage\n",
    "process = psutil.Process()\n",
    "memory_usage_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Model size\n",
    "model_size_mb = os.path.getsize('bilstm/bilstm_attention_model.h5') / (1024 * 1024)\n",
    "\n",
    "print(\"2. PERFORMANCE METRICS:\")\n",
    "print(f\"   - Estimated Training Time: {estimated_training_time:.2f} seconds\")\n",
    "print(f\"   - Average Inference Time (100 samples): {avg_inference_time:.4f} seconds\")\n",
    "print(f\"   - Inference Time per Sample: {inference_time_per_sample*1000:.2f} ms\")\n",
    "print(f\"   - Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"   - Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "performance_metrics = {\n",
    "    'estimated_training_time_seconds': float(estimated_training_time),\n",
    "    'avg_inference_time_100_samples': float(avg_inference_time),\n",
    "    'inference_time_per_sample_ms': float(inference_time_per_sample * 1000),\n",
    "    'memory_usage_mb': float(memory_usage_mb),\n",
    "    'model_size_mb': float(model_size_mb),\n",
    "    'epochs_trained': int(epochs_trained)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eb9bb",
   "metadata": {},
   "source": [
    "#### 12.3 Kh·∫£ nƒÉng t·ªïng qu√°t h√≥a (Generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19adc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GENERALIZATION METRICS\n",
    "\n",
    "# Train vs Validation performance\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "# Overfitting indicators\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "loss_gap = final_val_loss - final_train_loss\n",
    "\n",
    "# Training stability (variance in last few epochs)\n",
    "last_5_val_acc = history.history['val_accuracy'][-5:]\n",
    "val_acc_variance = np.var(last_5_val_acc) if len(last_5_val_acc) >= 5 else 0\n",
    "\n",
    "# Best validation accuracy\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "\n",
    "print(\"3. GENERALIZATION METRICS:\")\n",
    "print(f\"   - Final Train Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"   - Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"   - Accuracy Gap (Train-Val): {acc_gap:.4f}\")\n",
    "print(f\"   - Loss Gap (Val-Train): {loss_gap:.4f}\")\n",
    "print(f\"   - Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_val_acc_epoch})\")\n",
    "print(f\"   - Validation Accuracy Variance (last 5): {val_acc_variance:.6f}\")\n",
    "\n",
    "# Overfitting assessment\n",
    "if acc_gap > 0.05:\n",
    "    overfitting_status = \"Likely Overfitting\"\n",
    "elif acc_gap < -0.02:\n",
    "    overfitting_status = \"Possible Underfitting\"\n",
    "else:\n",
    "    overfitting_status = \"Good Generalization\"\n",
    "\n",
    "print(f\"   - Overfitting Assessment: {overfitting_status}\")\n",
    "\n",
    "generalization_metrics = {\n",
    "    'final_train_accuracy': float(final_train_acc),\n",
    "    'final_validation_accuracy': float(final_val_acc),\n",
    "    'final_train_loss': float(final_train_loss),\n",
    "    'final_validation_loss': float(final_val_loss),\n",
    "    'accuracy_gap': float(acc_gap),\n",
    "    'loss_gap': float(loss_gap),\n",
    "    'best_validation_accuracy': float(best_val_acc),\n",
    "    'best_validation_accuracy_epoch': int(best_val_acc_epoch),\n",
    "    'validation_accuracy_variance': float(val_acc_variance),\n",
    "    'overfitting_status': overfitting_status\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aeeef5",
   "metadata": {},
   "source": [
    "#### 12.4 ƒê·ªô ph·ª©c t·∫°p m√¥ h√¨nh (Model Complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MODEL COMPLEXITY METRICS\n",
    "\n",
    "# S·ªë parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "# S·ªë layers\n",
    "total_layers = len(model.layers)\n",
    "\n",
    "# FLOPS estimation (rough)\n",
    "def estimate_flops():\n",
    "    flops = 0\n",
    "    # Embedding layer\n",
    "    flops += MAX_FEATURES * EMBEDDING_DIM\n",
    "    # LSTM layers (approximation)\n",
    "    flops += 4 * LSTM_UNITS * EMBEDDING_DIM * MAXLEN * 2  # BiLSTM\n",
    "    flops += 4 * LSTM_UNITS * LSTM_UNITS * MAXLEN * 2     # Second BiLSTM\n",
    "    # Dense layers\n",
    "    flops += (LSTM_UNITS * 2) * 64 + 64 * 32 + 32 * 1\n",
    "    return flops\n",
    "\n",
    "estimated_flops = estimate_flops()\n",
    "\n",
    "print(\"4. MODEL COMPLEXITY METRICS:\")\n",
    "print(f\"   - Total Parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   - Non-trainable Parameters: {non_trainable_params:,}\")\n",
    "print(f\"   - Total Layers: {total_layers}\")\n",
    "print(f\"   - Estimated FLOPs: {estimated_flops:,}\")\n",
    "print(f\"   - Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Architecture details\n",
    "layer_details = []\n",
    "for i, layer in enumerate(model.layers):\n",
    "    layer_info = {\n",
    "        'layer_index': i,\n",
    "        'layer_name': layer.name,\n",
    "        'layer_type': type(layer).__name__,\n",
    "        'output_shape': str(layer.output_shape),\n",
    "        'param_count': layer.count_params()\n",
    "    }\n",
    "    layer_details.append(layer_info)\n",
    "\n",
    "complexity_metrics = {\n",
    "    'total_parameters': int(total_params),\n",
    "    'trainable_parameters': int(trainable_params),\n",
    "    'non_trainable_parameters': int(non_trainable_params),\n",
    "    'total_layers': int(total_layers),\n",
    "    'estimated_flops': int(estimated_flops),\n",
    "    'model_size_mb': float(model_size_mb),\n",
    "    'layer_details': layer_details\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458bd0b",
   "metadata": {},
   "source": [
    "#### 12.5 T√≠nh ·ªïn ƒë·ªãnh v√† ƒë·ªô tin c·∫≠y (Stability & Reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fe6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. STABILITY & RELIABILITY METRICS\n",
    "\n",
    "# Confidence analysis\n",
    "confidence_scores = np.abs(y_pred_prob.flatten() - 0.5)  # Distance from 0.5\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "confidence_std = np.std(confidence_scores)\n",
    "\n",
    "# High confidence predictions (> 0.3 distance from 0.5)\n",
    "high_confidence_mask = confidence_scores > 0.3\n",
    "high_confidence_accuracy = accuracy_score(y_test[high_confidence_mask], \n",
    "                                         y_pred[high_confidence_mask]) if np.any(high_confidence_mask) else 0\n",
    "\n",
    "# Low confidence predictions\n",
    "low_confidence_mask = confidence_scores <= 0.1\n",
    "low_confidence_count = np.sum(low_confidence_mask)\n",
    "low_confidence_accuracy = accuracy_score(y_test[low_confidence_mask], \n",
    "                                        y_pred[low_confidence_mask]) if np.any(low_confidence_mask) else 0\n",
    "\n",
    "# Error analysis\n",
    "correct_predictions = (y_pred == y_test)\n",
    "error_rate_by_confidence = []\n",
    "confidence_bins = np.linspace(0, 0.5, 6)\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (confidence_scores >= confidence_bins[i]) & (confidence_scores < confidence_bins[i+1])\n",
    "    if np.any(mask):\n",
    "        error_rate = 1 - accuracy_score(y_test[mask], y_pred[mask])\n",
    "        error_rate_by_confidence.append(error_rate)\n",
    "    else:\n",
    "        error_rate_by_confidence.append(0)\n",
    "\n",
    "print(\"5. STABILITY & RELIABILITY METRICS:\")\n",
    "print(f\"   - Average Confidence: {avg_confidence:.4f}\")\n",
    "print(f\"   - Confidence Std: {confidence_std:.4f}\")\n",
    "print(f\"   - High Confidence Accuracy (>0.3): {high_confidence_accuracy:.4f}\")\n",
    "print(f\"   - Low Confidence Count (‚â§0.1): {low_confidence_count}\")\n",
    "print(f\"   - Low Confidence Accuracy: {low_confidence_accuracy:.4f}\")\n",
    "\n",
    "reliability_metrics = {\n",
    "    'average_confidence': float(avg_confidence),\n",
    "    'confidence_std': float(confidence_std),\n",
    "    'high_confidence_accuracy': float(high_confidence_accuracy),\n",
    "    'low_confidence_count': int(low_confidence_count),\n",
    "    'low_confidence_accuracy': float(low_confidence_accuracy),\n",
    "    'error_rate_by_confidence_bins': [float(x) for x in error_rate_by_confidence]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb06873",
   "metadata": {},
   "source": [
    "### 13. V·∫Ω bi·ªÉu ƒë·ªì th·ªëng k√™ t·ªïng h·ª£p\n",
    "\n",
    "T·∫°o c√°c bi·ªÉu ƒë·ªì ƒë·ªÉ tr·ª±c quan h√≥a t·∫•t c·∫£ c√°c th√¥ng s·ªë ƒë√°nh gi√°:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE VISUALIZATION\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Accuracy Metrics Bar Chart\n",
    "ax1 = plt.subplot(3, 4, 1)\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "metrics_values = [accuracy, precision, recall, f1, auc_roc]\n",
    "bars = plt.bar(metrics_names, metrics_values, color=['#2E86C1', '#28B463', '#F39C12', '#E74C3C', '#8E44AD'])\n",
    "plt.title('Accuracy Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Confusion Matrix Heatmap\n",
    "ax2 = plt.subplot(3, 4, 2)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# 3. ROC Curve\n",
    "ax3 = plt.subplot(3, 4, 3)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_roc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# 4. Training History\n",
    "ax4 = plt.subplot(3, 4, 4)\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "plt.plot(epochs, history.history['accuracy'], 'bo-', label='Training Accuracy', markersize=4)\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'ro-', label='Validation Accuracy', markersize=4)\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Performance Metrics\n",
    "ax5 = plt.subplot(3, 4, 5)\n",
    "perf_names = ['Inference Time\\n(ms/sample)', 'Memory Usage\\n(MB)', 'Model Size\\n(MB)']\n",
    "perf_values = [inference_time_per_sample*1000, memory_usage_mb, model_size_mb]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = plt.bar(perf_names, perf_values, color=colors)\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Value')\n",
    "for bar, value in zip(bars, perf_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(perf_values)*0.01, \n",
    "             f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6. Model Complexity\n",
    "ax6 = plt.subplot(3, 4, 6)\n",
    "complexity_names = ['Parameters\\n(K)', 'Layers', 'FLOPs\\n(M)']\n",
    "complexity_values = [total_params/1000, total_layers, estimated_flops/1000000]\n",
    "bars = plt.bar(complexity_names, complexity_values, color=['#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "plt.title('Model Complexity')\n",
    "plt.ylabel('Count')\n",
    "for bar, value in zip(bars, complexity_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(complexity_values)*0.01, \n",
    "             f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 7. Confidence Distribution\n",
    "ax7 = plt.subplot(3, 4, 7)\n",
    "plt.hist(y_pred_prob, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 8. Error Rate by Confidence\n",
    "ax8 = plt.subplot(3, 4, 8)\n",
    "confidence_bin_centers = [(confidence_bins[i] + confidence_bins[i+1])/2 for i in range(len(confidence_bins)-1)]\n",
    "plt.plot(confidence_bin_centers, error_rate_by_confidence, 'ro-', linewidth=2, markersize=6)\n",
    "plt.title('Error Rate by Confidence Level')\n",
    "plt.xlabel('Confidence Level')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Train vs Validation Comparison\n",
    "ax9 = plt.subplot(3, 4, 9)\n",
    "comparison_metrics = ['Accuracy', 'Loss']\n",
    "train_values = [final_train_acc, final_train_loss]\n",
    "val_values = [final_val_acc, final_val_loss]\n",
    "x = np.arange(len(comparison_metrics))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, train_values, width, label='Training', color='lightblue')\n",
    "plt.bar(x + width/2, val_values, width, label='Validation', color='lightcoral')\n",
    "plt.title('Train vs Validation')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(x, comparison_metrics)\n",
    "plt.legend()\n",
    "\n",
    "# 10. Loss History\n",
    "ax10 = plt.subplot(3, 4, 10)\n",
    "plt.plot(epochs, history.history['loss'], 'bo-', label='Training Loss', markersize=4)\n",
    "plt.plot(epochs, history.history['val_loss'], 'ro-', label='Validation Loss', markersize=4)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. Classification Performance by Class\n",
    "ax11 = plt.subplot(3, 4, 11)\n",
    "class_metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "negative_scores = [tn/(tn+fp), tn/(tn+fn), 2*(tn/(tn+fp))*(tn/(tn+fn))/((tn/(tn+fp))+(tn/(tn+fn)))]\n",
    "positive_scores = [precision, recall, f1]\n",
    "x = np.arange(len(class_metrics))\n",
    "plt.bar(x - width/2, negative_scores, width, label='Negative Class', color='lightcoral')\n",
    "plt.bar(x + width/2, positive_scores, width, label='Positive Class', color='lightgreen')\n",
    "plt.title('Performance by Class')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(x, class_metrics)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 12. Model Size Breakdown\n",
    "ax12 = plt.subplot(3, 4, 12)\n",
    "param_breakdown = ['Trainable', 'Non-trainable']\n",
    "param_values = [trainable_params/1000, non_trainable_params/1000]\n",
    "colors = ['#FF9999', '#66B2FF']\n",
    "wedges, texts, autotexts = plt.pie(param_values, labels=param_breakdown, colors=colors, \n",
    "                                  autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Parameters Breakdown (K)')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.suptitle('BiLSTM + Attention Model - Comprehensive Analysis', fontsize=16, y=0.98)\n",
    "plt.savefig('bilstm/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Bi·ªÉu ƒë·ªì t·ªïng h·ª£p ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'bilstm/comprehensive_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc62c17",
   "metadata": {},
   "source": [
    "### 14. L∆∞u t·∫•t c·∫£ c√°c th√¥ng s·ªë ƒë·ªÉ so s√°nh v·ªõi BERT\n",
    "\n",
    "T·ªïng h·ª£p v√† l∆∞u t·∫•t c·∫£ c√°c metrics v√†o file JSON ƒë·ªÉ so s√°nh v·ªõi m√¥ h√¨nh BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·ªîNG H·ª¢P T·∫§T C·∫¢ METRICS ƒê·ªÇ SO S√ÅNH\n",
    "comprehensive_metrics = {\n",
    "    'model_info': {\n",
    "        'model_name': 'BiLSTM + Attention',\n",
    "        'dataset': 'IMDb Movie Reviews',\n",
    "        'task': 'Binary Sentiment Classification',\n",
    "        'framework': 'TensorFlow/Keras',\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'hyperparameters': {\n",
    "            'max_features': MAX_FEATURES,\n",
    "            'max_length': MAXLEN,\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'lstm_units': LSTM_UNITS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'epochs': EPOCHS\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # 1. Accuracy Metrics\n",
    "    'accuracy_metrics': accuracy_metrics,\n",
    "    \n",
    "    # 2. Performance Metrics  \n",
    "    'performance_metrics': performance_metrics,\n",
    "    \n",
    "    # 3. Generalization Metrics\n",
    "    'generalization_metrics': generalization_metrics,\n",
    "    \n",
    "    # 4. Model Complexity\n",
    "    'complexity_metrics': complexity_metrics,\n",
    "    \n",
    "    # 5. Reliability Metrics\n",
    "    'reliability_metrics': reliability_metrics,\n",
    "    \n",
    "    # 6. Training History\n",
    "    'training_history': {\n",
    "        'train_accuracy_history': [float(x) for x in history.history['accuracy']],\n",
    "        'val_accuracy_history': [float(x) for x in history.history['val_accuracy']],\n",
    "        'train_loss_history': [float(x) for x in history.history['loss']],\n",
    "        'val_loss_history': [float(x) for x in history.history['val_loss']]\n",
    "    },\n",
    "    \n",
    "    # 7. Additional Analysis\n",
    "    'additional_metrics': {\n",
    "        'dataset_size': {\n",
    "            'train_samples': len(x_train),\n",
    "            'test_samples': len(x_test),\n",
    "            'total_samples': len(x_train) + len(x_test)\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'train_positive': int(np.sum(y_train)),\n",
    "            'train_negative': int(len(y_train) - np.sum(y_train)),\n",
    "            'test_positive': int(np.sum(y_test)),\n",
    "            'test_negative': int(len(y_test) - np.sum(y_test))\n",
    "        },\n",
    "        'prediction_stats': {\n",
    "            'predicted_positive': int(np.sum(y_pred)),\n",
    "            'predicted_negative': int(len(y_pred) - np.sum(y_pred)),\n",
    "            'avg_positive_confidence': float(np.mean(y_pred_prob[y_test == 1])),\n",
    "            'avg_negative_confidence': float(np.mean(1 - y_pred_prob[y_test == 0]))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# L∆∞u v√†o file JSON\n",
    "with open('bilstm/bilstm_comprehensive_metrics.json', 'w') as f:\n",
    "    json.dump(comprehensive_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ T·∫•t c·∫£ metrics ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'bilstm/bilstm_comprehensive_metrics.json'\")\n",
    "\n",
    "# T·∫°o summary report\n",
    "summary_report = f'''\n",
    "=================================================================\n",
    "            BILSTM + ATTENTION MODEL - SUMMARY REPORT\n",
    "=================================================================\n",
    "\n",
    "üìä MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "   ‚Ä¢ Precision: {precision:.4f}\n",
    "   ‚Ä¢ Recall: {recall:.4f}\n",
    "   ‚Ä¢ F1-Score: {f1:.4f}\n",
    "   ‚Ä¢ AUC-ROC: {auc_roc:.4f}\n",
    "\n",
    "‚ö° PERFORMANCE:\n",
    "   ‚Ä¢ Training Time: ~{estimated_training_time:.0f} seconds\n",
    "   ‚Ä¢ Inference Time: {inference_time_per_sample*1000:.2f} ms/sample\n",
    "   ‚Ä¢ Model Size: {model_size_mb:.2f} MB\n",
    "   ‚Ä¢ Memory Usage: {memory_usage_mb:.2f} MB\n",
    "\n",
    "üîß MODEL COMPLEXITY:\n",
    "   ‚Ä¢ Total Parameters: {total_params:,}\n",
    "   ‚Ä¢ Trainable Parameters: {trainable_params:,}\n",
    "   ‚Ä¢ Total Layers: {total_layers}\n",
    "   ‚Ä¢ Estimated FLOPs: {estimated_flops:,}\n",
    "\n",
    "üìà GENERALIZATION:\n",
    "   ‚Ä¢ Best Validation Accuracy: {best_val_acc:.4f}\n",
    "   ‚Ä¢ Overfitting Status: {overfitting_status}\n",
    "   ‚Ä¢ Train-Val Accuracy Gap: {acc_gap:.4f}\n",
    "\n",
    "üéØ RELIABILITY:\n",
    "   ‚Ä¢ Average Confidence: {avg_confidence:.4f}\n",
    "   ‚Ä¢ High Confidence Accuracy: {high_confidence_accuracy:.4f}\n",
    "   ‚Ä¢ Low Confidence Predictions: {low_confidence_count}\n",
    "\n",
    "=================================================================\n",
    "Model v√† t·∫•t c·∫£ metrics ƒë√£ ƒë∆∞·ª£c l∆∞u trong th∆∞ m·ª•c 'bilstm/'\n",
    "S·∫µn s√†ng ƒë·ªÉ so s√°nh v·ªõi BERT!\n",
    "=================================================================\n",
    "'''\n",
    "print(summary_report)\n",
    "# L∆∞u summary report\n",
    "with open('bilstm/summary_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "print(\"‚úÖ Summary report ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o 'bilstm/summary_report.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0f064",
   "metadata": {},
   "source": [
    "### 15. T·∫°o script so s√°nh v·ªõi BERT\n",
    "\n",
    "T·∫°o s·∫µn script ƒë·ªÉ so s√°nh v·ªõi m√¥ h√¨nh BERT khi c√≥ d·ªØ li·ªáu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫†O SCRIPT SO S√ÅNH V·ªöI BERT\n",
    "comparison_script = '''\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_models(bilstm_metrics_path, bert_metrics_path):\n",
    "    \"\"\"\n",
    "    So s√°nh BiLSTM v·ªõi BERT t·ª´ c√°c file metrics JSON\n",
    "    \"\"\"\n",
    "    # Load metrics\n",
    "    with open(bilstm_metrics_path, 'r') as f:\n",
    "        bilstm_metrics = json.load(f)\n",
    "    \n",
    "    with open(bert_metrics_path, 'r') as f:\n",
    "        bert_metrics = json.load(f)\n",
    "    \n",
    "    # T·∫°o b·∫£ng so s√°nh\n",
    "    comparison_data = {\n",
    "        'Metric': [\n",
    "            'Test Accuracy',\n",
    "            'Precision', \n",
    "            'Recall',\n",
    "            'F1-Score',\n",
    "            'AUC-ROC',\n",
    "            'Model Size (MB)',\n",
    "            'Parameters',\n",
    "            'Inference Time (ms)',\n",
    "            'Training Time (s)',\n",
    "            'Memory Usage (MB)'\n",
    "        ],\n",
    "        'BiLSTM + Attention': [\n",
    "            bilstm_metrics['accuracy_metrics']['accuracy'],\n",
    "            bilstm_metrics['accuracy_metrics']['precision'],\n",
    "            bilstm_metrics['accuracy_metrics']['recall'],\n",
    "            bilstm_metrics['accuracy_metrics']['f1_score'],\n",
    "            bilstm_metrics['accuracy_metrics']['auc_roc'],\n",
    "            bilstm_metrics['performance_metrics']['model_size_mb'],\n",
    "            bilstm_metrics['complexity_metrics']['total_parameters'],\n",
    "            bilstm_metrics['performance_metrics']['inference_time_per_sample_ms'],\n",
    "            bilstm_metrics['performance_metrics']['estimated_training_time_seconds'],\n",
    "            bilstm_metrics['performance_metrics']['memory_usage_mb']\n",
    "        ],\n",
    "        'BERT': [\n",
    "            # S·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn khi c√≥ d·ªØ li·ªáu BERT\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('accuracy', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('precision', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('recall', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('f1_score', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('auc_roc', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('model_size_mb', 0),\n",
    "            bert_metrics.get('complexity_metrics', {}).get('total_parameters', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('inference_time_per_sample_ms', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('training_time_seconds', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('memory_usage_mb', 0)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"MODEL COMPARISON TABLE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy metrics\n",
    "    accuracy_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "    bilstm_acc = df.iloc[0:5]['BiLSTM + Attention'].values\n",
    "    bert_acc = df.iloc[0:5]['BERT'].values\n",
    "    \n",
    "    x = np.arange(len(accuracy_metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, bilstm_acc, width, label='BiLSTM', color='lightblue')\n",
    "    axes[0,0].bar(x + width/2, bert_acc, width, label='BERT', color='lightcoral')\n",
    "    axes[0,0].set_title('Accuracy Metrics Comparison')\n",
    "    axes[0,0].set_xlabel('Metrics')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(accuracy_metrics, rotation=45)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    \n",
    "    # Performance metrics\n",
    "    perf_metrics = ['Model Size (MB)', 'Inference Time (ms)', 'Memory Usage (MB)']\n",
    "    bilstm_perf = [df.iloc[5]['BiLSTM + Attention'], df.iloc[7]['BiLSTM + Attention'], df.iloc[9]['BiLSTM + Attention']]\n",
    "    bert_perf = [df.iloc[5]['BERT'], df.iloc[7]['BERT'], df.iloc[9]['BERT']]\n",
    "    \n",
    "    x = np.arange(len(perf_metrics))\n",
    "    axes[0,1].bar(x - width/2, bilstm_perf, width, label='BiLSTM', color='lightgreen')\n",
    "    axes[0,1].bar(x + width/2, bert_perf, width, label='BERT', color='orange')\n",
    "    axes[0,1].set_title('Performance Metrics Comparison')\n",
    "    axes[0,1].set_xlabel('Metrics')  \n",
    "    axes[0,1].set_ylabel('Value')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(perf_metrics, rotation=45)\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Parameters comparison\n",
    "    models = ['BiLSTM + Attention', 'BERT']\n",
    "    params = [df.iloc[6]['BiLSTM + Attention'], df.iloc[6]['BERT']]\n",
    "    \n",
    "    axes[1,0].bar(models, params, color=['purple', 'gold'])\n",
    "    axes[1,0].set_title('Model Parameters Comparison')\n",
    "    axes[1,0].set_ylabel('Number of Parameters')\n",
    "    \n",
    "    # Training time comparison\n",
    "    train_times = [df.iloc[8]['BiLSTM + Attention'], df.iloc[8]['BERT']]\n",
    "    \n",
    "    axes[1,1].bar(models, train_times, color=['teal', 'crimson'])\n",
    "    axes[1,1].set_title('Training Time Comparison')\n",
    "    axes[1,1].set_ylabel('Training Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# S·ª≠ d·ª•ng:\n",
    "# df = compare_models('bilstm/bilstm_comprehensive_metrics.json', 'bert/bert_comprehensive_metrics.json')\n",
    "'''\n",
    "\n",
    "# L∆∞u script\n",
    "with open('bilstm/compare_models.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(comparison_script)\n",
    "\n",
    "print(\"‚úÖ Script so s√°nh ƒë√£ ƒë∆∞·ª£c t·∫°o: 'bilstm/compare_models.py'\")\n",
    "print(\"\\nüìã Files ƒë√£ ƒë∆∞·ª£c t·∫°o trong th∆∞ m·ª•c 'bilstm/':\")\n",
    "print(\"   ‚Ä¢ bilstm_attention_model.h5 - M√¥ h√¨nh ƒë√£ train\")\n",
    "print(\"   ‚Ä¢ word_index.pkl - Tokenizer info\")\n",
    "print(\"   ‚Ä¢ bilstm_comprehensive_metrics.json - T·∫•t c·∫£ metrics\")\n",
    "print(\"   ‚Ä¢ summary_report.txt - B√°o c√°o t√≥m t·∫Øt\")\n",
    "print(\"   ‚Ä¢ comprehensive_analysis.png - Bi·ªÉu ƒë·ªì t·ªïng h·ª£p\")\n",
    "print(\"   ‚Ä¢ compare_models.py - Script so s√°nh v·ªõi BERT\")\n",
    "\n",
    "print(\"\\nüîÑ ƒê·ªÉ so s√°nh v·ªõi BERT sau n√†y, ch·∫°y:\")\n",
    "print(\"   from bilstm.compare_models import compare_models\")\n",
    "print(\"   compare_models('bilstm/bilstm_comprehensive_metrics.json', 'bert_metrics.json')\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th√¥ng tin th∆∞ m·ª•c\n",
    "import os\n",
    "print(f\"\\nüìÅ N·ªôi dung th∆∞ m·ª•c 'bilstm':\")\n",
    "for file in os.listdir('bilstm'):\n",
    "    file_path = os.path.join('bilstm', file)\n",
    "    size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"   ‚Ä¢ {file} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ch·∫°y th·ª≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ CH∆Ø∆†NG TR√åNH PH√ÇN T√çCH C·∫¢M X√öC REVIEW PHIM\n",
      "S·ª≠ d·ª•ng BiLSTM + Attention Model\n",
      "============================================================\n",
      "üöÄ ƒêANG KH·ªûI T·∫†O H·ªÜ TH·ªêNG PH√ÇN T√çCH C·∫¢M X√öC...\n",
      "============================================================\n",
      "üìÇ ƒêang load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully from ./bilstm_attention_imdb_model.h5\n",
      "üìö ƒêang load t·ª´ ƒëi·ªÉn...\n",
      "‚úÖ T·ª´ ƒëi·ªÉn c√≥ 88584 t·ª´\n",
      "\n",
      "============================================================\n",
      "üé¨ H·ªÜ TH·ªêNG PH√ÇN T√çCH C·∫¢M X√öC REVIEW PHIM\n",
      "============================================================\n",
      "üìù Nh·∫≠p review phim ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c\n",
      "üè∑Ô∏è  K·∫øt qu·∫£: 0 = Ti√™u c·ª±c | 1 = T√≠ch c·ª±c\n",
      "‚å®Ô∏è  G√µ 'quit', 'exit' ho·∫∑c 'q' ƒë·ªÉ tho√°t\n",
      "============================================================\n",
      "üîç ƒêang ph√¢n t√≠ch...\n",
      "üìÑ Text: it good\n",
      "üéØ C·∫£m x√∫c: Ti√™u c·ª±c (Negative)\n",
      "üè∑Ô∏è  Nh√£n: 0\n",
      "üìä ƒê·ªô tin c·∫≠y: 0.9538 (95.38%)\n",
      "üî¢ ƒêi·ªÉm s·ªë raw: 0.0462\n",
      "--------------------------------------------------\n",
      "üîç ƒêang ph√¢n t√≠ch...\n",
      "üìÑ Text: 0\n",
      "üéØ C·∫£m x√∫c: Ti√™u c·ª±c (Negative)\n",
      "üè∑Ô∏è  Nh√£n: 0\n",
      "üìä ƒê·ªô tin c·∫≠y: 0.9473 (94.73%)\n",
      "üî¢ ƒêi·ªÉm s·ªë raw: 0.0527\n",
      "--------------------------------------------------\n",
      "üëã T·∫°m bi·ªát! C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Attention Layer cho BiLSTM\n",
    "    Cho ph√©p m√¥ h√¨nh t·∫≠p trung v√†o c√°c ph·∫ßn quan tr·ªçng trong chu·ªói ƒë·∫ßu v√†o\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Ma tr·∫≠n tr·ªçng s·ªë attention\n",
    "        self.W = self.add_weight(name=\"attention_weight\", \n",
    "                                shape=(input_shape[-1], 1),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        # Bias cho attention\n",
    "        self.b = self.add_weight(name=\"attention_bias\", \n",
    "                                shape=(input_shape[1], 1),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, time_steps, features)\n",
    "        # T√≠nh attention scores\n",
    "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        # Chu·∫©n h√≥a attention weights b·∫±ng softmax\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        \n",
    "        # √Åp d·ª•ng attention weights l√™n input\n",
    "        output = x * a\n",
    "        \n",
    "        # T·ªïng h·ª£p th√¥ng tin t·ª´ t·∫•t c·∫£ time steps\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "def load_trained_model(model_path):\n",
    "    \"\"\"\n",
    "    Load model BiLSTM + Attention ƒë√£ train\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load model v·ªõi custom AttentionLayer\n",
    "        custom_objects = {'AttentionLayer': AttentionLayer}\n",
    "        model = load_model(model_path, custom_objects=custom_objects)\n",
    "        print(f\"‚úÖ Model loaded successfully from {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        \n",
    "        # Th·ª≠ c√°ch kh√°c v·ªõi custom_object_scope\n",
    "        try:\n",
    "            with tf.keras.utils.custom_object_scope({'AttentionLayer': AttentionLayer}):\n",
    "                model = load_model(model_path)\n",
    "                print(f\"‚úÖ Model loaded with custom_object_scope\")\n",
    "                return model\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Second attempt failed: {e2}\")\n",
    "            return None\n",
    "\n",
    "def predict_sentiment(model, text, word_index, maxlen=500):\n",
    "    \"\"\"\n",
    "    D·ª± ƒëo√°n c·∫£m x√∫c cho m·ªôt text m·ªõi\n",
    "    \"\"\"\n",
    "    # Chuy·ªÉn text th√†nh sequence\n",
    "    sequence = []\n",
    "    for word in text.lower().split():\n",
    "        if word in word_index and word_index[word] < 10000:\n",
    "            sequence.append(word_index[word])\n",
    "    \n",
    "    # N·∫øu sequence r·ªóng, return th√¥ng b√°o\n",
    "    if not sequence:\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω text n√†y (kh√¥ng c√≥ t·ª´ n√†o trong t·ª´ ƒëi·ªÉn)\")\n",
    "        print(\"-\" * 50)\n",
    "        return None, None, None\n",
    "    \n",
    "    # Padding\n",
    "    sequence = pad_sequences([sequence], maxlen=maxlen)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sequence, verbose=0)[0][0]\n",
    "    \n",
    "    # X√°c ƒë·ªãnh sentiment v√† confidence\n",
    "    if prediction > 0.5:\n",
    "        sentiment = \"T√≠ch c·ª±c (Positive)\"\n",
    "        label = 1\n",
    "        confidence = prediction\n",
    "    else:\n",
    "        sentiment = \"Ti√™u c·ª±c (Negative)\"\n",
    "        label = 0\n",
    "        confidence = 1 - prediction\n",
    "    \n",
    "    return sentiment, label, confidence, prediction\n",
    "\n",
    "def interactive_sentiment_analysis():\n",
    "    \"\"\"\n",
    "    Ch∆∞∆°ng tr√¨nh ch√≠nh - nh·∫≠p text t·ª´ b√†n ph√≠m v√† d·ª± ƒëo√°n\n",
    "    \"\"\"\n",
    "    # ƒê∆∞·ªùng d·∫´n model\n",
    "    model_path = \"./bilstm_attention_imdb_model.h5\"\n",
    "    maxlen = 500  # Chi·ªÅu d√†i sequence t·ªëi ƒëa\n",
    "    \n",
    "    print(\"üöÄ ƒêANG KH·ªûI T·∫†O H·ªÜ TH·ªêNG PH√ÇN T√çCH C·∫¢M X√öC...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load model\n",
    "    print(\"üìÇ ƒêang load model...\")\n",
    "    model = load_trained_model(model_path)\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"‚ùå Kh√¥ng th·ªÉ load model. Vui l√≤ng ki·ªÉm tra:\")\n",
    "        print(\"1. ƒê∆∞·ªùng d·∫´n file model c√≥ ƒë√∫ng kh√¥ng?\")\n",
    "        print(\"2. File model c√≥ t·ªìn t·∫°i kh√¥ng?\")\n",
    "        print(\"3. Model c√≥ ƒë∆∞·ª£c train v·ªõi AttentionLayer kh√¥ng?\")\n",
    "        return\n",
    "    \n",
    "    # Load word index t·ª´ IMDB dataset\n",
    "    print(\"üìö ƒêang load t·ª´ ƒëi·ªÉn...\")\n",
    "    word_index = imdb.get_word_index()\n",
    "    print(f\"‚úÖ T·ª´ ƒëi·ªÉn c√≥ {len(word_index)} t·ª´\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé¨ H·ªÜ TH·ªêNG PH√ÇN T√çCH C·∫¢M X√öC REVIEW PHIM\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"üìù Nh·∫≠p review phim ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c\")\n",
    "    print(\"üè∑Ô∏è  K·∫øt qu·∫£: 0 = Ti√™u c·ª±c | 1 = T√≠ch c·ª±c\")\n",
    "    print(\"‚å®Ô∏è  G√µ 'quit', 'exit' ho·∫∑c 'q' ƒë·ªÉ tho√°t\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # V√≤ng l·∫∑p ch√≠nh\n",
    "    while True:\n",
    "        try:\n",
    "            # Nh·∫≠p text t·ª´ b√†n ph√≠m\n",
    "            text = input(\"\\nüìù Nh·∫≠p review: \").strip()\n",
    "            \n",
    "            # Ki·ªÉm tra l·ªánh tho√°t\n",
    "            if text.lower() in ['quit', 'exit', 'q', 'tho√°t']:\n",
    "                print(\"üëã T·∫°m bi·ªát! C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng h·ªá th·ªëng.\")\n",
    "                break\n",
    "            \n",
    "            # Ki·ªÉm tra input r·ªóng\n",
    "            if not text:\n",
    "                print(\"‚ö†Ô∏è  Vui l√≤ng nh·∫≠p review kh√¥ng r·ªóng!\")\n",
    "                continue\n",
    "            \n",
    "            # D·ª± ƒëo√°n\n",
    "            print(\"üîç ƒêang ph√¢n t√≠ch...\")\n",
    "            result = predict_sentiment(model, text, word_index, maxlen)\n",
    "            \n",
    "            if result[0] is not None:  # N·∫øu predict th√†nh c√¥ng\n",
    "                sentiment, label, confidence, raw_score = result\n",
    "                \n",
    "                # Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "                print(f\"üìÑ Text: {text}\")\n",
    "                print(f\"üéØ C·∫£m x√∫c: {sentiment}\")\n",
    "                print(f\"üè∑Ô∏è  Nh√£n: {label}\")\n",
    "                print(f\"üìä ƒê·ªô tin c·∫≠y: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "                print(f\"üî¢ ƒêi·ªÉm s·ªë raw: {raw_score:.4f}\")\n",
    "                print(\"-\" * 50)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Ch∆∞∆°ng tr√¨nh b·ªã ng·∫Øt. T·∫°m bi·ªát!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói kh√¥ng mong mu·ªën: {e}\")\n",
    "            print(\"üîÑ Vui l√≤ng th·ª≠ l·∫°i...\")\n",
    "\n",
    "def test_with_sample_reviews():\n",
    "    \"\"\"\n",
    "    Test v·ªõi c√°c reviews m·∫´u\n",
    "    \"\"\"\n",
    "    model_path = \"./bilstm_attention_imdb_model.h5\"\n",
    "    maxlen = 500\n",
    "    \n",
    "    # Load model v√† word_index\n",
    "    model = load_trained_model(model_path)\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    word_index = imdb.get_word_index()\n",
    "    \n",
    "    print(\"üß™ KI·ªÇM TH·ª¨ V·ªöI C√ÅC REVIEWS M·∫™U\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Test v·ªõi c√°c examples kh√°c nhau\n",
    "    test_texts = [\n",
    "        \"This movie is absolutely fantastic! Great acting and amazing storyline.\",\n",
    "        \"Terrible film. Boring plot and bad acting. Complete waste of time.\",\n",
    "        \"The movie was okay, nothing special but not bad either.\",\n",
    "        \"One of the best movies I have ever seen! Highly recommended!\",\n",
    "        \"Awful movie. Poor direction and terrible script. Very disappointed.\",\n",
    "        \"Amazing cinematography and outstanding performances from all actors.\",\n",
    "        \"I fell asleep halfway through. Very boring and predictable plot.\",\n",
    "        \"Perfect movie for the weekend. Really enjoyed watching it.\",\n",
    "        \"Not my cup of tea but can understand why others might like it.\",\n",
    "        \"Masterpiece! Every scene was perfectly crafted and emotionally engaging.\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\nüìù Test {i}:\")\n",
    "        result = predict_sentiment(model, text, word_index, maxlen)\n",
    "        \n",
    "        if result[0] is not None:\n",
    "            sentiment, label, confidence, raw_score = result\n",
    "            print(f\"üìÑ Text: {text}\")\n",
    "            print(f\"üéØ C·∫£m x√∫c: {sentiment}\")\n",
    "            print(f\"üè∑Ô∏è  Nh√£n: {label}\")\n",
    "            print(f\"üìä ƒê·ªô tin c·∫≠y: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "            print(f\"üî¢ ƒêi·ªÉm s·ªë raw: {raw_score:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üé¨ CH∆Ø∆†NG TR√åNH PH√ÇN T√çCH C·∫¢M X√öC REVIEW PHIM\")\n",
    "    print(\"S·ª≠ d·ª•ng BiLSTM + Attention Model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    choice = input(\"Ch·ªçn ch·∫ø ƒë·ªô:\\n1. Nh·∫≠p t·ª´ b√†n ph√≠m (1)\\n2. Test v·ªõi reviews m·∫´u (2)\\nL·ª±a ch·ªçn (1/2): \").strip()\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        test_with_sample_reviews()\n",
    "    else:\n",
    "        interactive_sentiment_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imdb-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
