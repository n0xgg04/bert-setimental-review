{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân loại cảm xúc IMDB sử dụng BiLSTM + Attention\n",
    "\n",
    "Trong notebook này, chúng ta sẽ xây dựng mô hình BiLSTM kết hợp với lớp Attention để phân loại cảm xúc (tích cực/tiêu cực) trên bộ dữ liệu IMDb movie reviews.\n",
    "\n",
    "## Mục tiêu:\n",
    "- Xây dựng mô hình BiLSTM + Attention từ đầu\n",
    "- Huấn luyện trên dataset IMDb \n",
    "- Đánh giá hiệu quả mô hình\n",
    "- So sánh với các phương pháp khác\n",
    "\n",
    "## Các bước thực hiện:\n",
    "1. Import thư viện và thiết lập môi trường\n",
    "2. Load và tiền xử lý dữ liệu IMDb\n",
    "3. Xây dựng lớp Attention tùy chỉnh\n",
    "4. Tạo mô hình BiLSTM + Attention\n",
    "5. Huấn luyện mô hình\n",
    "6. Đánh giá và trực quan hóa kết quả\n",
    "7. Test với dữ liệu mới"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ae569",
   "metadata": {},
   "source": [
    "### 1. Import thư viện và thiết lập môi trường\n",
    "\n",
    "Import các thư viện cần thiết cho việc xây dựng mô hình BiLSTM + Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d145113a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "GPU available: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thiết lập seed cho reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8f5b7",
   "metadata": {},
   "source": [
    "### 2. Tạo lớp Attention tùy chỉnh\n",
    "\n",
    "Xây dựng lớp Attention để tăng khả năng tập trung vào những phần quan trọng trong chuỗi văn bản:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c620a2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lớp AttentionLayer đã được định nghĩa thành công!\n"
     ]
    }
   ],
   "source": [
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Attention Layer cho BiLSTM\n",
    "    Cho phép mô hình tập trung vào các phần quan trọng trong chuỗi đầu vào\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Ma trận trọng số attention\n",
    "        self.W = self.add_weight(name=\"attention_weight\", \n",
    "                                shape=(input_shape[-1], 1),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        # Bias cho attention\n",
    "        self.b = self.add_weight(name=\"attention_bias\", \n",
    "                                shape=(input_shape[1], 1),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)        \n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # x shape: (batch_size, time_steps, features)\n",
    "        # Tính attention scores\n",
    "        e = tf.nn.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n",
    "        # Chuẩn hóa attention weights bằng softmax\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        \n",
    "        # Áp dụng attention weights lên input\n",
    "        output = x * a\n",
    "        \n",
    "        # Tổng hợp thông tin từ tất cả time steps\n",
    "        return tf.reduce_sum(output, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "print(\"Lớp AttentionLayer đã được định nghĩa thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168b18c",
   "metadata": {},
   "source": [
    "### 3. Load và tiền xử lý dữ liệu IMDb\n",
    "\n",
    "Tải bộ dữ liệu IMDb movie reviews và thực hiện tiền xử lý:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ae3055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu IMDb...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 12s 1us/step\n",
      "Số lượng reviews training: 25000\n",
      "Số lượng reviews testing: 25000\n",
      "Số lượng từ tối đa: 10000\n",
      "Độ dài sequence tối đa: 500\n",
      "\n",
      "Phân bố nhãn training:\n",
      "- Negative (0): 12500 reviews\n",
      "- Positive (1): 12500 reviews\n",
      "\n",
      "Đang padding sequences...\n",
      "Shape sau padding:\n",
      "x_train: (25000, 500)\n",
      "x_test: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "def load_and_preprocess_data(max_features=10000, maxlen=500):\n",
    "    \"\"\"\n",
    "    Load và tiền xử lý dữ liệu IMDb\n",
    "    \"\"\"\n",
    "    print(\"Đang tải dữ liệu IMDb...\")\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "    \n",
    "    print(f\"Số lượng reviews training: {len(x_train)}\")\n",
    "    print(f\"Số lượng reviews testing: {len(x_test)}\")\n",
    "    print(f\"Số lượng từ tối đa: {max_features}\")\n",
    "    print(f\"Độ dài sequence tối đa: {maxlen}\")\n",
    "    \n",
    "    # Kiểm tra phân bố nhãn\n",
    "    print(f\"\\nPhân bố nhãn training:\")\n",
    "    print(f\"- Negative (0): {sum(y_train == 0)} reviews\")\n",
    "    print(f\"- Positive (1): {sum(y_train == 1)} reviews\")\n",
    "    \n",
    "    # Padding sequences để có cùng độ dài\n",
    "    print(\"\\nĐang padding sequences...\")\n",
    "    x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "    x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "    \n",
    "    print(f\"Shape sau padding:\")\n",
    "    print(f\"x_train: {x_train.shape}\")\n",
    "    print(f\"x_test: {x_test.shape}\")\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Thiết lập tham số\n",
    "MAX_FEATURES = 10000  # Số từ vựng tối đa\n",
    "MAXLEN = 500         # Độ dài sequence tối đa\n",
    "\n",
    "# Load dữ liệu\n",
    "(x_train, y_train), (x_test, y_test) = load_and_preprocess_data(MAX_FEATURES, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddeae7c",
   "metadata": {},
   "source": [
    "### 4. Xây dựng mô hình BiLSTM + Attention\n",
    "\n",
    "Tạo mô hình sử dụng lớp BiLSTM kết hợp với Attention mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98a9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xây dựng model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kiến trúc mô hình:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 128)          1280000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 128)          0         \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 500, 128)          98816     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 500, 128)          98816     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " attention_layer (Attention  (None, 128)               628       \n",
      " Layer)                                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1488629 (5.68 MB)\n",
      "Trainable params: 1488629 (5.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_bilstm_attention_model(max_features, maxlen, embedding_dim=128, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Tạo model BiLSTM với Attention\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Lớp Embedding: chuyển từ số thành vector đặc trưng\n",
    "    model.add(layers.Embedding(max_features, embedding_dim, input_length=maxlen))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    \n",
    "    # Hai lớp BiLSTM để học đặc trưng từ cả hai hướng\n",
    "    model.add(layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
    "    ))\n",
    "    model.add(layers.Bidirectional(\n",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
    "    ))\n",
    "    \n",
    "    # Lớp Attention để tập trung vào phần quan trọng\n",
    "    model.add(AttentionLayer())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Các lớp Dense để phân loại\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    # Lớp output: phân loại nhị phân (positive/negative)\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Tham số mô hình\n",
    "EMBEDDING_DIM = 128  # Dimension của embedding\n",
    "LSTM_UNITS = 64      # Số units trong LSTM\n",
    "\n",
    "# Tạo mô hình\n",
    "print(\"Đang xây dựng model...\")\n",
    "model = create_bilstm_attention_model(MAX_FEATURES, MAXLEN, EMBEDDING_DIM, LSTM_UNITS)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Hiển thị kiến trúc mô hình\n",
    "print(\"\\nKiến trúc mô hình:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e695efa",
   "metadata": {},
   "source": [
    "### 5. Huấn luyện mô hình\n",
    "\n",
    "Thiết lập callbacks và bắt đầu quá trình huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd08769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bắt đầu huấn luyện mô hình...\n",
      "Batch size: 32\n",
      "Max epochs: 10\n",
      "Callbacks: EarlyStopping, ReduceLROnPlateau\n",
      "Epoch 1/10\n",
      "221/782 [=======>......................] - ETA: 7:49 - loss: 0.6513 - accuracy: 0.5703"
     ]
    }
   ],
   "source": [
    "# Tham số huấn luyện\n",
    "BATCH_SIZE = 32      # Batch size\n",
    "EPOCHS = 10          # Số epochs\n",
    "\n",
    "# Thiết lập callbacks để tối ưu hóa quá trình huấn luyện\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "# Bắt đầu huấn luyện\n",
    "print(\"Bắt đầu huấn luyện mô hình...\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Callbacks: EarlyStopping, ReduceLROnPlateau\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nHuấn luyện hoàn tất!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202f23e",
   "metadata": {},
   "source": [
    "### 6. Trực quan hóa quá trình huấn luyện\n",
    "\n",
    "Vẽ biểu đồ để quan sát quá trình huấn luyện:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32547282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Vẽ biểu đồ quá trình training\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    ax1.set_title('Độ chính xác mô hình qua các epoch')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    ax2.set_title('Loss mô hình qua các epoch')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e99231b",
   "metadata": {},
   "source": [
    "### 7. Đánh giá mô hình\n",
    "\n",
    "Đánh giá hiệu quả mô hình trên tập test và hiển thị các metrics chi tiết:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe450fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Đánh giá model và hiển thị kết quả chi tiết\n",
    "    \"\"\"\n",
    "    # Dự đoán trên tập test\n",
    "    print(\"Đang thực hiện dự đoán trên tập test...\")\n",
    "    y_pred_prob = model.predict(x_test)\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BÁO CÁO PHÂN LOẠI CHI TIẾT\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Negative', 'Positive'], \n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    plt.title('Ma trận nhầm lẫn (Confusion Matrix)')\n",
    "    plt.xlabel('Dự đoán')\n",
    "    plt.ylabel('Thực tế')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test accuracy\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"KẾT QUẢ CUỐI CÙNG\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Độ chính xác trên tập test: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "    print(f\"Loss trên tập test: {test_loss:.4f}\")\n",
    "    \n",
    "    return y_pred, y_pred_prob\n",
    "\n",
    "# Đánh giá mô hình\n",
    "y_pred, y_pred_prob = evaluate_model(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9044dedc",
   "metadata": {},
   "source": [
    "### 8. Test với dữ liệu mới\n",
    "\n",
    "Thử nghiệm mô hình với một số reviews mẫu để xem khả năng dự đoán:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd39bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, text, word_index, maxlen=500):\n",
    "    \"\"\"\n",
    "    Dự đoán cảm xúc cho một text mới\n",
    "    \"\"\"\n",
    "    # Chuyển text thành sequence\n",
    "    sequence = []\n",
    "    for word in text.lower().split():\n",
    "        if word in word_index and word_index[word] < 10000:\n",
    "            sequence.append(word_index[word])\n",
    "    \n",
    "    # Nếu sequence rỗng, return unknown\n",
    "    if not sequence:\n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"Không thể xử lý text này (không có từ nào trong từ điển)\")\n",
    "        print(\"-\" * 50)\n",
    "        return\n",
    "    \n",
    "    # Padding\n",
    "    sequence = pad_sequences([sequence], maxlen=maxlen)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(sequence, verbose=0)[0][0]\n",
    "    \n",
    "    sentiment = \"Tích cực (Positive)\" if prediction > 0.5 else \"Tiêu cực (Negative)\"\n",
    "    confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Cảm xúc: {sentiment}\")\n",
    "    print(f\"Độ tin cậy: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(f\"Điểm số raw: {prediction:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Load word index để chuyển đổi text\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "print(\"KIỂM THỬ MÔ HÌNH VỚI CÁC REVIEWS MẪU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test với các examples khác nhau\n",
    "test_texts = [\n",
    "    \"This movie is absolutely fantastic! Great acting and amazing storyline.\",\n",
    "    \"Terrible film. Boring plot and bad acting. Complete waste of time.\",\n",
    "    \"The movie was okay, nothing special but not bad either.\",\n",
    "    \"One of the best movies I have ever seen! Highly recommended!\",\n",
    "    \"Awful movie. Poor direction and terrible script. Very disappointed.\",\n",
    "    \"Amazing cinematography and outstanding performances from all actors.\",\n",
    "    \"I fell asleep halfway through. Very boring and predictable plot.\",\n",
    "    \"Perfect movie for the weekend. Really enjoyed watching it.\",\n",
    "    \"Not my cup of tea but can understand why others might like it.\",\n",
    "    \"Masterpiece! Every scene was perfectly crafted and emotionally engaging.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    predict_sentiment(model, text, word_index, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f885f",
   "metadata": {},
   "source": [
    "### 9. Lưu mô hình\n",
    "\n",
    "Lưu lại mô hình đã huấn luyện để sử dụng sau này:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu mô hình\n",
    "model.save('bilstm_attention_imdb_model.h5')\n",
    "print(\"✅ Mô hình đã được lưu thành 'bilstm_attention_imdb_model.h5'\")\n",
    "\n",
    "# Lưu thông tin về quá trình huấn luyện\n",
    "import json\n",
    "\n",
    "training_info = {\n",
    "    'max_features': MAX_FEATURES,\n",
    "    'maxlen': MAXLEN,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs_trained': len(history.history['loss']),\n",
    "    'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "    'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "    'final_train_loss': float(history.history['loss'][-1]),\n",
    "    'final_val_loss': float(history.history['val_loss'][-1])\n",
    "}\n",
    "\n",
    "with open('training_info.json', 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"✅ Thông tin huấn luyện đã được lưu vào 'training_info.json'\")\n",
    "print(\"\\nThông tin mô hình:\")\n",
    "for key, value in training_info.items():\n",
    "    if 'accuracy' in key or 'loss' in key:\n",
    "        print(f\"- {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"- {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4b412",
   "metadata": {},
   "source": [
    "### 10. Tóm tắt và so sánh\n",
    "\n",
    "#### Ưu điểm của mô hình BiLSTM + Attention:\n",
    "- **Hiểu ngữ cảnh hai chiều**: BiLSTM xử lý thông tin từ cả hai hướng (trước và sau)\n",
    "- **Tập trung vào phần quan trọng**: Lớp Attention giúp mô hình chú ý đến những từ quan trọng nhất\n",
    "- **Hiệu quả với dữ liệu tuần tự**: Phù hợp với đặc tính của văn bản là dữ liệu tuần tự\n",
    "- **Tốc độ huấn luyện hợp lý**: Nhanh hơn so với các mô hình Transformer lớn\n",
    "\n",
    "#### Nhược điểm:\n",
    "- **Độ chính xác thấp hơn BERT**: Có thể không đạt được độ chính xác cao như các mô hình pretrained lớn\n",
    "- **Cần nhiều dữ liệu**: Hiệu quả tốt nhất khi có lượng dữ liệu huấn luyện lớn\n",
    "- **Khó xử lý mối quan hệ xa**: LSTM có thể gặp khó khăn với các phụ thuộc xa trong văn bản\n",
    "\n",
    "#### So sánh với BERT:\n",
    "- **BERT**: Độ chính xác cao hơn nhưng cần nhiều tài nguyên tính toán và thời gian huấn luyện\n",
    "- **BiLSTM + Attention**: Cân bằng giữa hiệu quả và tốc độ, phù hợp khi tài nguyên hạn chế\n",
    "\n",
    "Mô hình BiLSTM + Attention là một lựa chọn tốt khi cần cân bằng giữa độ chính xác và hiệu quả tính toán!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4360a",
   "metadata": {},
   "source": [
    "### 11. Tạo thư mục và lưu mô hình\n",
    "\n",
    "Tạo thư mục `bilstm` và lưu mô hình cùng với các thông tin liên quan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Tạo thư mục bilstm nếu chưa có\n",
    "os.makedirs('bilstm', exist_ok=True)\n",
    "\n",
    "# Lưu mô hình vào thư mục bilstm\n",
    "model.save('bilstm/bilstm_attention_model.h5')\n",
    "print(\"✅ Mô hình đã được lưu vào 'bilstm/bilstm_attention_model.h5'\")\n",
    "\n",
    "# Lưu tokenizer info (word_index) để sử dụng sau này\n",
    "import pickle\n",
    "with open('bilstm/word_index.pkl', 'wb') as f:\n",
    "    pickle.dump(word_index, f)\n",
    "print(\"✅ Word index đã được lưu vào 'bilstm/word_index.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f366e6",
   "metadata": {},
   "source": [
    "### 12. Tính toán các thông số đánh giá chi tiết\n",
    "\n",
    "#### 12.1 Độ chính xác (Accuracy Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0a6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Tính toán các metrics chính xác\n",
    "print(\"Đang tính toán các thông số đánh giá...\")\n",
    "\n",
    "# 1. ACCURACY METRICS\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Confusion Matrix chi tiết\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Tính toán thêm các metrics\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)  # = recall\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "false_negative_rate = fn / (fn + tp)\n",
    "\n",
    "print(\"1. ACCURACY METRICS:\")\n",
    "print(f\"   - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   - Precision: {precision:.4f}\")\n",
    "print(f\"   - Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"   - Specificity: {specificity:.4f}\")\n",
    "print(f\"   - F1-Score: {f1:.4f}\")\n",
    "print(f\"   - AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"   - False Positive Rate: {false_positive_rate:.4f}\")\n",
    "print(f\"   - False Negative Rate: {false_negative_rate:.4f}\")\n",
    "\n",
    "accuracy_metrics = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'specificity': float(specificity),\n",
    "    'f1_score': float(f1),\n",
    "    'auc_roc': float(auc_roc),\n",
    "    'false_positive_rate': float(false_positive_rate),\n",
    "    'false_negative_rate': float(false_negative_rate),\n",
    "    'true_positives': int(tp),\n",
    "    'true_negatives': int(tn),\n",
    "    'false_positives': int(fp),\n",
    "    'false_negatives': int(fn)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9100d1af",
   "metadata": {},
   "source": [
    "#### 12.2 Hiệu suất tính toán (Performance Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10283157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PERFORMANCE METRICS\n",
    "\n",
    "# Tính thời gian training (ước lượng từ history)\n",
    "epochs_trained = len(history.history['loss'])\n",
    "# Ước lượng thời gian training (có thể không chính xác 100%)\n",
    "estimated_training_time = epochs_trained * 60  # Ước lượng 60s/epoch\n",
    "\n",
    "# Đo thời gian inference\n",
    "print(\"Đang đo thời gian inference...\")\n",
    "inference_times = []\n",
    "batch_size_test = 100\n",
    "\n",
    "for i in range(5):  # Đo 5 lần để lấy trung bình\n",
    "    start_time = time.time()\n",
    "    _ = model.predict(x_test[:batch_size_test], verbose=0)\n",
    "    end_time = time.time()\n",
    "    inference_times.append(end_time - start_time)\n",
    "\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "inference_time_per_sample = avg_inference_time / batch_size_test\n",
    "\n",
    "# Đo memory usage\n",
    "process = psutil.Process()\n",
    "memory_usage_mb = process.memory_info().rss / 1024 / 1024\n",
    "\n",
    "# Model size\n",
    "model_size_mb = os.path.getsize('bilstm/bilstm_attention_model.h5') / (1024 * 1024)\n",
    "\n",
    "print(\"2. PERFORMANCE METRICS:\")\n",
    "print(f\"   - Estimated Training Time: {estimated_training_time:.2f} seconds\")\n",
    "print(f\"   - Average Inference Time (100 samples): {avg_inference_time:.4f} seconds\")\n",
    "print(f\"   - Inference Time per Sample: {inference_time_per_sample*1000:.2f} ms\")\n",
    "print(f\"   - Memory Usage: {memory_usage_mb:.2f} MB\")\n",
    "print(f\"   - Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "performance_metrics = {\n",
    "    'estimated_training_time_seconds': float(estimated_training_time),\n",
    "    'avg_inference_time_100_samples': float(avg_inference_time),\n",
    "    'inference_time_per_sample_ms': float(inference_time_per_sample * 1000),\n",
    "    'memory_usage_mb': float(memory_usage_mb),\n",
    "    'model_size_mb': float(model_size_mb),\n",
    "    'epochs_trained': int(epochs_trained)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eb9bb",
   "metadata": {},
   "source": [
    "#### 12.3 Khả năng tổng quát hóa (Generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19adc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. GENERALIZATION METRICS\n",
    "\n",
    "# Train vs Validation performance\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "# Overfitting indicators\n",
    "acc_gap = final_train_acc - final_val_acc\n",
    "loss_gap = final_val_loss - final_train_loss\n",
    "\n",
    "# Training stability (variance in last few epochs)\n",
    "last_5_val_acc = history.history['val_accuracy'][-5:]\n",
    "val_acc_variance = np.var(last_5_val_acc) if len(last_5_val_acc) >= 5 else 0\n",
    "\n",
    "# Best validation accuracy\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_val_acc_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "\n",
    "print(\"3. GENERALIZATION METRICS:\")\n",
    "print(f\"   - Final Train Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"   - Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"   - Accuracy Gap (Train-Val): {acc_gap:.4f}\")\n",
    "print(f\"   - Loss Gap (Val-Train): {loss_gap:.4f}\")\n",
    "print(f\"   - Best Validation Accuracy: {best_val_acc:.4f} (Epoch {best_val_acc_epoch})\")\n",
    "print(f\"   - Validation Accuracy Variance (last 5): {val_acc_variance:.6f}\")\n",
    "\n",
    "# Overfitting assessment\n",
    "if acc_gap > 0.05:\n",
    "    overfitting_status = \"Likely Overfitting\"\n",
    "elif acc_gap < -0.02:\n",
    "    overfitting_status = \"Possible Underfitting\"\n",
    "else:\n",
    "    overfitting_status = \"Good Generalization\"\n",
    "\n",
    "print(f\"   - Overfitting Assessment: {overfitting_status}\")\n",
    "\n",
    "generalization_metrics = {\n",
    "    'final_train_accuracy': float(final_train_acc),\n",
    "    'final_validation_accuracy': float(final_val_acc),\n",
    "    'final_train_loss': float(final_train_loss),\n",
    "    'final_validation_loss': float(final_val_loss),\n",
    "    'accuracy_gap': float(acc_gap),\n",
    "    'loss_gap': float(loss_gap),\n",
    "    'best_validation_accuracy': float(best_val_acc),\n",
    "    'best_validation_accuracy_epoch': int(best_val_acc_epoch),\n",
    "    'validation_accuracy_variance': float(val_acc_variance),\n",
    "    'overfitting_status': overfitting_status\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1aeeef5",
   "metadata": {},
   "source": [
    "#### 12.4 Độ phức tạp mô hình (Model Complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MODEL COMPLEXITY METRICS\n",
    "\n",
    "# Số parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "# Số layers\n",
    "total_layers = len(model.layers)\n",
    "\n",
    "# FLOPS estimation (rough)\n",
    "def estimate_flops():\n",
    "    flops = 0\n",
    "    # Embedding layer\n",
    "    flops += MAX_FEATURES * EMBEDDING_DIM\n",
    "    # LSTM layers (approximation)\n",
    "    flops += 4 * LSTM_UNITS * EMBEDDING_DIM * MAXLEN * 2  # BiLSTM\n",
    "    flops += 4 * LSTM_UNITS * LSTM_UNITS * MAXLEN * 2     # Second BiLSTM\n",
    "    # Dense layers\n",
    "    flops += (LSTM_UNITS * 2) * 64 + 64 * 32 + 32 * 1\n",
    "    return flops\n",
    "\n",
    "estimated_flops = estimate_flops()\n",
    "\n",
    "print(\"4. MODEL COMPLEXITY METRICS:\")\n",
    "print(f\"   - Total Parameters: {total_params:,}\")\n",
    "print(f\"   - Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   - Non-trainable Parameters: {non_trainable_params:,}\")\n",
    "print(f\"   - Total Layers: {total_layers}\")\n",
    "print(f\"   - Estimated FLOPs: {estimated_flops:,}\")\n",
    "print(f\"   - Model Size: {model_size_mb:.2f} MB\")\n",
    "\n",
    "# Architecture details\n",
    "layer_details = []\n",
    "for i, layer in enumerate(model.layers):\n",
    "    layer_info = {\n",
    "        'layer_index': i,\n",
    "        'layer_name': layer.name,\n",
    "        'layer_type': type(layer).__name__,\n",
    "        'output_shape': str(layer.output_shape),\n",
    "        'param_count': layer.count_params()\n",
    "    }\n",
    "    layer_details.append(layer_info)\n",
    "\n",
    "complexity_metrics = {\n",
    "    'total_parameters': int(total_params),\n",
    "    'trainable_parameters': int(trainable_params),\n",
    "    'non_trainable_parameters': int(non_trainable_params),\n",
    "    'total_layers': int(total_layers),\n",
    "    'estimated_flops': int(estimated_flops),\n",
    "    'model_size_mb': float(model_size_mb),\n",
    "    'layer_details': layer_details\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458bd0b",
   "metadata": {},
   "source": [
    "#### 12.5 Tính ổn định và độ tin cậy (Stability & Reliability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24fe6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. STABILITY & RELIABILITY METRICS\n",
    "\n",
    "# Confidence analysis\n",
    "confidence_scores = np.abs(y_pred_prob.flatten() - 0.5)  # Distance from 0.5\n",
    "avg_confidence = np.mean(confidence_scores)\n",
    "confidence_std = np.std(confidence_scores)\n",
    "\n",
    "# High confidence predictions (> 0.3 distance from 0.5)\n",
    "high_confidence_mask = confidence_scores > 0.3\n",
    "high_confidence_accuracy = accuracy_score(y_test[high_confidence_mask], \n",
    "                                         y_pred[high_confidence_mask]) if np.any(high_confidence_mask) else 0\n",
    "\n",
    "# Low confidence predictions\n",
    "low_confidence_mask = confidence_scores <= 0.1\n",
    "low_confidence_count = np.sum(low_confidence_mask)\n",
    "low_confidence_accuracy = accuracy_score(y_test[low_confidence_mask], \n",
    "                                        y_pred[low_confidence_mask]) if np.any(low_confidence_mask) else 0\n",
    "\n",
    "# Error analysis\n",
    "correct_predictions = (y_pred == y_test)\n",
    "error_rate_by_confidence = []\n",
    "confidence_bins = np.linspace(0, 0.5, 6)\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (confidence_scores >= confidence_bins[i]) & (confidence_scores < confidence_bins[i+1])\n",
    "    if np.any(mask):\n",
    "        error_rate = 1 - accuracy_score(y_test[mask], y_pred[mask])\n",
    "        error_rate_by_confidence.append(error_rate)\n",
    "    else:\n",
    "        error_rate_by_confidence.append(0)\n",
    "\n",
    "print(\"5. STABILITY & RELIABILITY METRICS:\")\n",
    "print(f\"   - Average Confidence: {avg_confidence:.4f}\")\n",
    "print(f\"   - Confidence Std: {confidence_std:.4f}\")\n",
    "print(f\"   - High Confidence Accuracy (>0.3): {high_confidence_accuracy:.4f}\")\n",
    "print(f\"   - Low Confidence Count (≤0.1): {low_confidence_count}\")\n",
    "print(f\"   - Low Confidence Accuracy: {low_confidence_accuracy:.4f}\")\n",
    "\n",
    "reliability_metrics = {\n",
    "    'average_confidence': float(avg_confidence),\n",
    "    'confidence_std': float(confidence_std),\n",
    "    'high_confidence_accuracy': float(high_confidence_accuracy),\n",
    "    'low_confidence_count': int(low_confidence_count),\n",
    "    'low_confidence_accuracy': float(low_confidence_accuracy),\n",
    "    'error_rate_by_confidence_bins': [float(x) for x in error_rate_by_confidence]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb06873",
   "metadata": {},
   "source": [
    "### 13. Vẽ biểu đồ thống kê tổng hợp\n",
    "\n",
    "Tạo các biểu đồ để trực quan hóa tất cả các thông số đánh giá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE VISUALIZATION\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# 1. Accuracy Metrics Bar Chart\n",
    "ax1 = plt.subplot(3, 4, 1)\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "metrics_values = [accuracy, precision, recall, f1, auc_roc]\n",
    "bars = plt.bar(metrics_names, metrics_values, color=['#2E86C1', '#28B463', '#F39C12', '#E74C3C', '#8E44AD'])\n",
    "plt.title('Accuracy Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 1)\n",
    "for bar, value in zip(bars, metrics_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Confusion Matrix Heatmap\n",
    "ax2 = plt.subplot(3, 4, 2)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "# 3. ROC Curve\n",
    "ax3 = plt.subplot(3, 4, 3)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_roc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# 4. Training History\n",
    "ax4 = plt.subplot(3, 4, 4)\n",
    "epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "plt.plot(epochs, history.history['accuracy'], 'bo-', label='Training Accuracy', markersize=4)\n",
    "plt.plot(epochs, history.history['val_accuracy'], 'ro-', label='Validation Accuracy', markersize=4)\n",
    "plt.title('Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Performance Metrics\n",
    "ax5 = plt.subplot(3, 4, 5)\n",
    "perf_names = ['Inference Time\\n(ms/sample)', 'Memory Usage\\n(MB)', 'Model Size\\n(MB)']\n",
    "perf_values = [inference_time_per_sample*1000, memory_usage_mb, model_size_mb]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = plt.bar(perf_names, perf_values, color=colors)\n",
    "plt.title('Performance Metrics')\n",
    "plt.ylabel('Value')\n",
    "for bar, value in zip(bars, perf_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(perf_values)*0.01, \n",
    "             f'{value:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6. Model Complexity\n",
    "ax6 = plt.subplot(3, 4, 6)\n",
    "complexity_names = ['Parameters\\n(K)', 'Layers', 'FLOPs\\n(M)']\n",
    "complexity_values = [total_params/1000, total_layers, estimated_flops/1000000]\n",
    "bars = plt.bar(complexity_names, complexity_values, color=['#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "plt.title('Model Complexity')\n",
    "plt.ylabel('Count')\n",
    "for bar, value in zip(bars, complexity_values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(complexity_values)*0.01, \n",
    "             f'{value:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 7. Confidence Distribution\n",
    "ax7 = plt.subplot(3, 4, 7)\n",
    "plt.hist(y_pred_prob, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Decision Boundary')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 8. Error Rate by Confidence\n",
    "ax8 = plt.subplot(3, 4, 8)\n",
    "confidence_bin_centers = [(confidence_bins[i] + confidence_bins[i+1])/2 for i in range(len(confidence_bins)-1)]\n",
    "plt.plot(confidence_bin_centers, error_rate_by_confidence, 'ro-', linewidth=2, markersize=6)\n",
    "plt.title('Error Rate by Confidence Level')\n",
    "plt.xlabel('Confidence Level')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Train vs Validation Comparison\n",
    "ax9 = plt.subplot(3, 4, 9)\n",
    "comparison_metrics = ['Accuracy', 'Loss']\n",
    "train_values = [final_train_acc, final_train_loss]\n",
    "val_values = [final_val_acc, final_val_loss]\n",
    "x = np.arange(len(comparison_metrics))\n",
    "width = 0.35\n",
    "plt.bar(x - width/2, train_values, width, label='Training', color='lightblue')\n",
    "plt.bar(x + width/2, val_values, width, label='Validation', color='lightcoral')\n",
    "plt.title('Train vs Validation')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(x, comparison_metrics)\n",
    "plt.legend()\n",
    "\n",
    "# 10. Loss History\n",
    "ax10 = plt.subplot(3, 4, 10)\n",
    "plt.plot(epochs, history.history['loss'], 'bo-', label='Training Loss', markersize=4)\n",
    "plt.plot(epochs, history.history['val_loss'], 'ro-', label='Validation Loss', markersize=4)\n",
    "plt.title('Loss History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 11. Classification Performance by Class\n",
    "ax11 = plt.subplot(3, 4, 11)\n",
    "class_metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "negative_scores = [tn/(tn+fp), tn/(tn+fn), 2*(tn/(tn+fp))*(tn/(tn+fn))/((tn/(tn+fp))+(tn/(tn+fn)))]\n",
    "positive_scores = [precision, recall, f1]\n",
    "x = np.arange(len(class_metrics))\n",
    "plt.bar(x - width/2, negative_scores, width, label='Negative Class', color='lightcoral')\n",
    "plt.bar(x + width/2, positive_scores, width, label='Positive Class', color='lightgreen')\n",
    "plt.title('Performance by Class')\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(x, class_metrics)\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 12. Model Size Breakdown\n",
    "ax12 = plt.subplot(3, 4, 12)\n",
    "param_breakdown = ['Trainable', 'Non-trainable']\n",
    "param_values = [trainable_params/1000, non_trainable_params/1000]\n",
    "colors = ['#FF9999', '#66B2FF']\n",
    "wedges, texts, autotexts = plt.pie(param_values, labels=param_breakdown, colors=colors, \n",
    "                                  autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Parameters Breakdown (K)')\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.suptitle('BiLSTM + Attention Model - Comprehensive Analysis', fontsize=16, y=0.98)\n",
    "plt.savefig('bilstm/comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Biểu đồ tổng hợp đã được lưu vào 'bilstm/comprehensive_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc62c17",
   "metadata": {},
   "source": [
    "### 14. Lưu tất cả các thông số để so sánh với BERT\n",
    "\n",
    "Tổng hợp và lưu tất cả các metrics vào file JSON để so sánh với mô hình BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TỔNG HỢP TẤT CẢ METRICS ĐỂ SO SÁNH\n",
    "comprehensive_metrics = {\n",
    "    'model_info': {\n",
    "        'model_name': 'BiLSTM + Attention',\n",
    "        'dataset': 'IMDb Movie Reviews',\n",
    "        'task': 'Binary Sentiment Classification',\n",
    "        'framework': 'TensorFlow/Keras',\n",
    "        'training_date': datetime.now().isoformat(),\n",
    "        'hyperparameters': {\n",
    "            'max_features': MAX_FEATURES,\n",
    "            'max_length': MAXLEN,\n",
    "            'embedding_dim': EMBEDDING_DIM,\n",
    "            'lstm_units': LSTM_UNITS,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'epochs': EPOCHS\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # 1. Accuracy Metrics\n",
    "    'accuracy_metrics': accuracy_metrics,\n",
    "    \n",
    "    # 2. Performance Metrics  \n",
    "    'performance_metrics': performance_metrics,\n",
    "    \n",
    "    # 3. Generalization Metrics\n",
    "    'generalization_metrics': generalization_metrics,\n",
    "    \n",
    "    # 4. Model Complexity\n",
    "    'complexity_metrics': complexity_metrics,\n",
    "    \n",
    "    # 5. Reliability Metrics\n",
    "    'reliability_metrics': reliability_metrics,\n",
    "    \n",
    "    # 6. Training History\n",
    "    'training_history': {\n",
    "        'train_accuracy_history': [float(x) for x in history.history['accuracy']],\n",
    "        'val_accuracy_history': [float(x) for x in history.history['val_accuracy']],\n",
    "        'train_loss_history': [float(x) for x in history.history['loss']],\n",
    "        'val_loss_history': [float(x) for x in history.history['val_loss']]\n",
    "    },\n",
    "    \n",
    "    # 7. Additional Analysis\n",
    "    'additional_metrics': {\n",
    "        'dataset_size': {\n",
    "            'train_samples': len(x_train),\n",
    "            'test_samples': len(x_test),\n",
    "            'total_samples': len(x_train) + len(x_test)\n",
    "        },\n",
    "        'class_distribution': {\n",
    "            'train_positive': int(np.sum(y_train)),\n",
    "            'train_negative': int(len(y_train) - np.sum(y_train)),\n",
    "            'test_positive': int(np.sum(y_test)),\n",
    "            'test_negative': int(len(y_test) - np.sum(y_test))\n",
    "        },\n",
    "        'prediction_stats': {\n",
    "            'predicted_positive': int(np.sum(y_pred)),\n",
    "            'predicted_negative': int(len(y_pred) - np.sum(y_pred)),\n",
    "            'avg_positive_confidence': float(np.mean(y_pred_prob[y_test == 1])),\n",
    "            'avg_negative_confidence': float(np.mean(1 - y_pred_prob[y_test == 0]))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Lưu vào file JSON\n",
    "with open('bilstm/bilstm_comprehensive_metrics.json', 'w') as f:\n",
    "    json.dump(comprehensive_metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Tất cả metrics đã được lưu vào 'bilstm/bilstm_comprehensive_metrics.json'\")\n",
    "\n",
    "# Tạo summary report\n",
    "summary_report = f'''\n",
    "=================================================================\n",
    "            BILSTM + ATTENTION MODEL - SUMMARY REPORT\n",
    "=================================================================\n",
    "\n",
    "📊 MODEL PERFORMANCE:\n",
    "   • Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "   • Precision: {precision:.4f}\n",
    "   • Recall: {recall:.4f}\n",
    "   • F1-Score: {f1:.4f}\n",
    "   • AUC-ROC: {auc_roc:.4f}\n",
    "\n",
    "⚡ PERFORMANCE:\n",
    "   • Training Time: ~{estimated_training_time:.0f} seconds\n",
    "   • Inference Time: {inference_time_per_sample*1000:.2f} ms/sample\n",
    "   • Model Size: {model_size_mb:.2f} MB\n",
    "   • Memory Usage: {memory_usage_mb:.2f} MB\n",
    "\n",
    "🔧 MODEL COMPLEXITY:\n",
    "   • Total Parameters: {total_params:,}\n",
    "   • Trainable Parameters: {trainable_params:,}\n",
    "   • Total Layers: {total_layers}\n",
    "   • Estimated FLOPs: {estimated_flops:,}\n",
    "\n",
    "📈 GENERALIZATION:\n",
    "   • Best Validation Accuracy: {best_val_acc:.4f}\n",
    "   • Overfitting Status: {overfitting_status}\n",
    "   • Train-Val Accuracy Gap: {acc_gap:.4f}\n",
    "\n",
    "🎯 RELIABILITY:\n",
    "   • Average Confidence: {avg_confidence:.4f}\n",
    "   • High Confidence Accuracy: {high_confidence_accuracy:.4f}\n",
    "   • Low Confidence Predictions: {low_confidence_count}\n",
    "\n",
    "=================================================================\n",
    "Model và tất cả metrics đã được lưu trong thư mục 'bilstm/'\n",
    "Sẵn sàng để so sánh với BERT!\n",
    "=================================================================\n",
    "'''\n",
    "print(summary_report)\n",
    "# Lưu summary report\n",
    "with open('bilstm/summary_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "print(\"✅ Summary report đã được lưu vào 'bilstm/summary_report.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0f064",
   "metadata": {},
   "source": [
    "### 15. Tạo script so sánh với BERT\n",
    "\n",
    "Tạo sẵn script để so sánh với mô hình BERT khi có dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6880fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TẠO SCRIPT SO SÁNH VỚI BERT\n",
    "comparison_script = '''\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def compare_models(bilstm_metrics_path, bert_metrics_path):\n",
    "    \"\"\"\n",
    "    So sánh BiLSTM với BERT từ các file metrics JSON\n",
    "    \"\"\"\n",
    "    # Load metrics\n",
    "    with open(bilstm_metrics_path, 'r') as f:\n",
    "        bilstm_metrics = json.load(f)\n",
    "    \n",
    "    with open(bert_metrics_path, 'r') as f:\n",
    "        bert_metrics = json.load(f)\n",
    "    \n",
    "    # Tạo bảng so sánh\n",
    "    comparison_data = {\n",
    "        'Metric': [\n",
    "            'Test Accuracy',\n",
    "            'Precision', \n",
    "            'Recall',\n",
    "            'F1-Score',\n",
    "            'AUC-ROC',\n",
    "            'Model Size (MB)',\n",
    "            'Parameters',\n",
    "            'Inference Time (ms)',\n",
    "            'Training Time (s)',\n",
    "            'Memory Usage (MB)'\n",
    "        ],\n",
    "        'BiLSTM + Attention': [\n",
    "            bilstm_metrics['accuracy_metrics']['accuracy'],\n",
    "            bilstm_metrics['accuracy_metrics']['precision'],\n",
    "            bilstm_metrics['accuracy_metrics']['recall'],\n",
    "            bilstm_metrics['accuracy_metrics']['f1_score'],\n",
    "            bilstm_metrics['accuracy_metrics']['auc_roc'],\n",
    "            bilstm_metrics['performance_metrics']['model_size_mb'],\n",
    "            bilstm_metrics['complexity_metrics']['total_parameters'],\n",
    "            bilstm_metrics['performance_metrics']['inference_time_per_sample_ms'],\n",
    "            bilstm_metrics['performance_metrics']['estimated_training_time_seconds'],\n",
    "            bilstm_metrics['performance_metrics']['memory_usage_mb']\n",
    "        ],\n",
    "        'BERT': [\n",
    "            # Sẽ được điền khi có dữ liệu BERT\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('accuracy', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('precision', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('recall', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('f1_score', 0),\n",
    "            bert_metrics.get('accuracy_metrics', {}).get('auc_roc', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('model_size_mb', 0),\n",
    "            bert_metrics.get('complexity_metrics', {}).get('total_parameters', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('inference_time_per_sample_ms', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('training_time_seconds', 0),\n",
    "            bert_metrics.get('performance_metrics', {}).get('memory_usage_mb', 0)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    print(\"MODEL COMPARISON TABLE:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Vẽ biểu đồ so sánh\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Accuracy metrics\n",
    "    accuracy_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "    bilstm_acc = df.iloc[0:5]['BiLSTM + Attention'].values\n",
    "    bert_acc = df.iloc[0:5]['BERT'].values\n",
    "    \n",
    "    x = np.arange(len(accuracy_metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0,0].bar(x - width/2, bilstm_acc, width, label='BiLSTM', color='lightblue')\n",
    "    axes[0,0].bar(x + width/2, bert_acc, width, label='BERT', color='lightcoral')\n",
    "    axes[0,0].set_title('Accuracy Metrics Comparison')\n",
    "    axes[0,0].set_xlabel('Metrics')\n",
    "    axes[0,0].set_ylabel('Score')\n",
    "    axes[0,0].set_xticks(x)\n",
    "    axes[0,0].set_xticklabels(accuracy_metrics, rotation=45)\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].set_ylim(0, 1)\n",
    "    \n",
    "    # Performance metrics\n",
    "    perf_metrics = ['Model Size (MB)', 'Inference Time (ms)', 'Memory Usage (MB)']\n",
    "    bilstm_perf = [df.iloc[5]['BiLSTM + Attention'], df.iloc[7]['BiLSTM + Attention'], df.iloc[9]['BiLSTM + Attention']]\n",
    "    bert_perf = [df.iloc[5]['BERT'], df.iloc[7]['BERT'], df.iloc[9]['BERT']]\n",
    "    \n",
    "    x = np.arange(len(perf_metrics))\n",
    "    axes[0,1].bar(x - width/2, bilstm_perf, width, label='BiLSTM', color='lightgreen')\n",
    "    axes[0,1].bar(x + width/2, bert_perf, width, label='BERT', color='orange')\n",
    "    axes[0,1].set_title('Performance Metrics Comparison')\n",
    "    axes[0,1].set_xlabel('Metrics')  \n",
    "    axes[0,1].set_ylabel('Value')\n",
    "    axes[0,1].set_xticks(x)\n",
    "    axes[0,1].set_xticklabels(perf_metrics, rotation=45)\n",
    "    axes[0,1].legend()\n",
    "    \n",
    "    # Parameters comparison\n",
    "    models = ['BiLSTM + Attention', 'BERT']\n",
    "    params = [df.iloc[6]['BiLSTM + Attention'], df.iloc[6]['BERT']]\n",
    "    \n",
    "    axes[1,0].bar(models, params, color=['purple', 'gold'])\n",
    "    axes[1,0].set_title('Model Parameters Comparison')\n",
    "    axes[1,0].set_ylabel('Number of Parameters')\n",
    "    \n",
    "    # Training time comparison\n",
    "    train_times = [df.iloc[8]['BiLSTM + Attention'], df.iloc[8]['BERT']]\n",
    "    \n",
    "    axes[1,1].bar(models, train_times, color=['teal', 'crimson'])\n",
    "    axes[1,1].set_title('Training Time Comparison')\n",
    "    axes[1,1].set_ylabel('Training Time (seconds)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Sử dụng:\n",
    "# df = compare_models('bilstm/bilstm_comprehensive_metrics.json', 'bert/bert_comprehensive_metrics.json')\n",
    "'''\n",
    "\n",
    "# Lưu script\n",
    "with open('bilstm/compare_models.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(comparison_script)\n",
    "\n",
    "print(\"✅ Script so sánh đã được tạo: 'bilstm/compare_models.py'\")\n",
    "print(\"\\n📋 Files đã được tạo trong thư mục 'bilstm/':\")\n",
    "print(\"   • bilstm_attention_model.h5 - Mô hình đã train\")\n",
    "print(\"   • word_index.pkl - Tokenizer info\")\n",
    "print(\"   • bilstm_comprehensive_metrics.json - Tất cả metrics\")\n",
    "print(\"   • summary_report.txt - Báo cáo tóm tắt\")\n",
    "print(\"   • comprehensive_analysis.png - Biểu đồ tổng hợp\")\n",
    "print(\"   • compare_models.py - Script so sánh với BERT\")\n",
    "\n",
    "print(\"\\n🔄 Để so sánh với BERT sau này, chạy:\")\n",
    "print(\"   from bilstm.compare_models import compare_models\")\n",
    "print(\"   compare_models('bilstm/bilstm_comprehensive_metrics.json', 'bert_metrics.json')\")\n",
    "\n",
    "# Hiển thị thông tin thư mục\n",
    "import os\n",
    "print(f\"\\n📁 Nội dung thư mục 'bilstm':\")\n",
    "for file in os.listdir('bilstm'):\n",
    "    file_path = os.path.join('bilstm', file)\n",
    "    size = os.path.getsize(file_path) / 1024  # KB\n",
    "    print(f\"   • {file} ({size:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imdb-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
