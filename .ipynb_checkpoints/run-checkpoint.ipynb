{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Phân tích cảm xúc dựa trên đánh giá - Nhóm 5\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1b49a",
   "metadata": {},
   "source": [
    "Kiểm tra và sử dụng GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0f251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "GPU name: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f534974e",
   "metadata": {},
   "source": [
    "Nạp datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bab20009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error importing huggingface_hub.hf_api: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.hf_api: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.hf_api: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.repocard: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.repocard_data: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.hf_api: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n",
      "Error importing huggingface_hub.hf_api: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'CommitInfo' from 'huggingface_hub' (/Users/n0x/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/huggingface_hub/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/datasets/arrow_dataset.py:63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     CommitInfo,\n\u001b[1;32m     65\u001b[0m     CommitOperationAdd,\n\u001b[1;32m     66\u001b[0m     CommitOperationDelete,\n\u001b[1;32m     67\u001b[0m     DatasetCard,\n\u001b[1;32m     68\u001b[0m     DatasetCardData,\n\u001b[1;32m     69\u001b[0m     HfApi,\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhf_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RepoFile\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'CommitInfo' from 'huggingface_hub' (/Users/n0x/anaconda3/envs/imdb-sentiment/lib/python3.8/site-packages/huggingface_hub/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a38023",
   "metadata": {},
   "source": [
    "##### Nạp mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5574a18",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"imdb-finetuned\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"imdb-finetuned\")\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef1e8f1",
   "metadata": {},
   "source": [
    "Đọc review từ file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41ee59cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = input(\"Nhập review: \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e92c31",
   "metadata": {},
   "source": [
    "Tokenize các chuỗi review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8ce581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40517792",
   "metadata": {},
   "source": [
    "Dùng mô hình đã train, dự đoán cảm xúc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7460f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c4a96",
   "metadata": {},
   "source": [
    "In kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84aa3acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "sentiment = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "print(f\"Review sentiment: {sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imdb-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
